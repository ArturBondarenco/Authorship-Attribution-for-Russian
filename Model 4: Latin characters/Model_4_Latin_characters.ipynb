{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data"
      ],
      "metadata": {
        "id": "sEsO5HaT_z36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing libraries, downloading the model"
      ],
      "metadata": {
        "id": "bSP5RzSq_9Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import sklearn\n",
        "import numpy\n",
        "import spacy\n",
        "import string\n",
        "import sys\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(pandas.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(numpy.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPPcRPNYAAxL",
        "outputId": "1321755b-f4b0-45ce-a860-fc31366486a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.3\n",
            "1.2.2\n",
            "1.22.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small Russian model:\n",
        "# !python -m spacy download ru_core_news_sm\n",
        "# nlp = spacy.load('ru_core_news_sm')\n",
        "\n",
        "# Large Russian model:\n",
        "!python -m spacy download ru_core_news_lg\n",
        "nlp = spacy.load('ru_core_news_lg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_umxI11NAD8y",
        "outputId": "c94d7a78-7275-405c-b0c0-87a32a92787c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-15 21:40:28.268684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_lg-3.5.0/ru_core_news_lg-3.5.0-py3-none-any.whl (513.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.4/513.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-lg==3.5.0) (3.5.2)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-lg==3.5.0)\n",
            "  Downloading pymorphy3-1.2.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.1.2)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=694fa8f6d504ff892c5cf2d558253b1e53e5b793d81c00fa4944bf36c17c9fc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy3-dicts-ru, docopt, dawg-python, pymorphy3, ru-core-news-lg\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy3-1.2.0 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Making lists and doc objects from csv files"
      ],
      "metadata": {
        "id": "6wrhl23RAR8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the training data from a csv file\n",
        "train_set = pandas.read_csv('./train_data.csv', encoding='utf-8')\n",
        "# train_set"
      ],
      "metadata": {
        "id": "WEvAXn-cAHNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = pandas.read_csv('./test_data.csv', encoding='utf-8')\n",
        "# test_set"
      ],
      "metadata": {
        "id": "ZRmRub0rAgx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_set['text'].to_list()\n",
        "train_authors = train_set['author'].to_list()\n",
        "\n",
        "test_sentences = test_set['text'].to_list()\n",
        "test_authors = test_set['author'].to_list()\n",
        "\n",
        "print(len(train_authors), len(test_authors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xdlD_4SBKdJ",
        "outputId": "47335edf-d3e5-4912-a67d-540fa8cb56b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "test_doc_sentences = nlp.pipe(test_sentences)"
      ],
      "metadata": {
        "id": "a9wT4AbzBVxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the feature set"
      ],
      "metadata": {
        "id": "mSGg7NMxBcAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will import the Latin letters using 'string.ascii_letters'."
      ],
      "metadata": {
        "id": "wWhlY54rKPEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.ascii_letters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf3cseBTBZA5",
        "outputId": "a15a294c-f234-4441-ca77-1ba4616a8ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will check if additionaly French, German or Ukrainian special characters appear in the sentences, since all four writers were known to be using words from these languages."
      ],
      "metadata": {
        "id": "HoABCYTuKXS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "french_accent_marks = \"éèêëàÉÈÊËÀ\"\n",
        "german_characters = \"äöüßÄÖÜ\"\n",
        "ukrainian_characters = \"ґєїіҐЄЇІ\"\n",
        "\n",
        "for sentence in train_sentences:\n",
        "  for character in sentence:\n",
        "    if character in ukrainian_characters:\n",
        "      print('Ukr character found')\n",
        "    if character in german_characters:\n",
        "      print('German character found')\n",
        "    if character in french_accent_marks:\n",
        "      print('French character found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zui9razCF-_P",
        "outputId": "7c41e629-f7b7-42d1-af12-64bfb90e0317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n",
            "French character found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there are no special Ukrainian or German characters, but there are special French characters. Thus we will add them to our character string."
      ],
      "metadata": {
        "id": "TKXQ00V_MSu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_string = string.ascii_letters + french_accent_marks\n",
        "print(len(new_string), new_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdsmemGuKHZL",
        "outputId": "3c2a0d3a-ff4a-4147-8076-f99764016bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62 abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZéèêëàÉÈÊËÀ\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We are creating a matrix with zero vectors for each review (in training set and test set)\n",
        "train_features_matrix = numpy.zeros((len(train_sentences), len(new_string)))\n",
        "print(train_features_matrix.shape)\n",
        "\n",
        "test_features_matrix = numpy.zeros((len(test_sentences), len(new_string)))\n",
        "print(test_features_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFywLULGMkvP",
        "outputId": "abb96ecc-ef94-4fab-94e9-920f37611eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 62)\n",
            "(1000, 62)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying the feature vectors"
      ],
      "metadata": {
        "id": "Fi6LKX-8MzKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation tests"
      ],
      "metadata": {
        "id": "_UOOqMIZM4o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "\n",
        "for sentence, author in zip(train_sentences, train_authors):\n",
        "    for char in sentence:\n",
        "      if char in new_string:\n",
        "        print('Author:', author)\n",
        "        print(sentence)\n",
        "        char_id = new_string.index(char)\n",
        "        print(char_id)\n",
        "        break\n",
        "    counter +=1\n",
        "    if counter == 40:\n",
        "      break\n",
        "        # sys.exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDwu5zTRMrs4",
        "outputId": "61c911b2-3f37-4245-f59c-b195ef69b757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Tolstoy\n",
            "Oh, sans doute, c'est la plus charmante femme du monde, — сказала Анна Павловна с улыбкой над своей восторженностью.\n",
            "40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation test 2:"
      ],
      "metadata": {
        "id": "Qnoz7OEONxvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "\n",
        "for sentence, author, feature_vector in zip(train_sentences, train_authors, train_features_matrix):\n",
        "    for char in sentence:\n",
        "      if char in new_string:\n",
        "        print('Author:', author)\n",
        "        print(sentence)\n",
        "        char_id = new_string.index(char)\n",
        "        print(char_id)\n",
        "        feature_vector[char_id] = 1\n",
        "        print(feature_vector.tolist())\n",
        "        # sys.exit()\n",
        "    counter +=1\n",
        "    if counter == 40:\n",
        "      break"
      ],
      "metadata": {
        "id": "-9xofdYgM9yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing a function for vector modification"
      ],
      "metadata": {
        "id": "8tBFUgJyPMxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_feature_vectors(sentences, features_matrix):\n",
        "  for sentence, feature_vector in zip(sentences, features_matrix):\n",
        "    for char in sentence:\n",
        "      if char in new_string:\n",
        "        char_id = new_string.index(char)\n",
        "        feature_vector[char_id] = 1\n",
        "  return features_matrix"
      ],
      "metadata": {
        "id": "Mgst2uBGO6Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_matrix = numpy.zeros((len(train_sentences), len(new_string)))\n",
        "train_features_matrix_final = modify_feature_vectors(train_sentences, train_features_matrix)\n",
        "\n",
        "print(train_features_matrix_final[35:40])"
      ],
      "metadata": {
        "id": "QLI2pqu4PoFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "1g5PwCe5QJGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_latin = LogisticRegression()\n",
        "\n",
        "# Train the model on the data, storing the information learned from the dat`a\n",
        "# Model is learning the relationship between digits (x_train) and labels (y_train)\n",
        "lr_latin.fit(train_features_matrix_final, train_authors)\n",
        "\n",
        "print(lr_latin.classes_)\n",
        "print(lr_latin.get_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkUkUGv0QLVp",
        "outputId": "5fa4235a-9758-43f9-fed2-5f51f254a2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chekhov' 'Dostoevsky' 'Gogol' 'Tolstoy']\n",
            "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying the test set feature vectors"
      ],
      "metadata": {
        "id": "dzocjblnQqhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_features_matrix = numpy.zeros((len(test_sentences), len(new_string)))\n",
        "test_features_matrix_final = modify_feature_vectors(test_sentences, test_features_matrix)\n",
        "\n",
        "print(test_features_matrix_final[35:40])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nugt4FCoQnPd",
        "outputId": "72b7b97e-23a2-4f2d-9191-110afc064a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making predictions"
      ],
      "metadata": {
        "id": "21G-SGAcRJgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(i):\n",
        "    print(test_sentences[i])\n",
        "    # print the features of the index\n",
        "    print(test_features_matrix_final[i])\n",
        "    # print the correct label of the index\n",
        "    print(test_authors[i])\n",
        "\n",
        "    print()\n",
        "    print(\"Prediction:\")\n",
        "    # print the prediction for the features of this index\n",
        "    print(lr_latin.predict([test_features_matrix_final[i]]))\n",
        "    # print the probabilities for each label predictions\n",
        "    print(lr_latin.predict_proba([test_features_matrix_final[i]]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "3-zcrjYvQ5L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(0)\n",
        "predict(28)\n",
        "predict(39)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYc0Zmv8RUuF",
        "outputId": "c5f447a7-ffc2-49d9-d889-e18b2f481b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Фома Фомич, говорю, разве это возможное дело?\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Dostoevsky\n",
            "\n",
            "Prediction:\n",
            "['Chekhov']\n",
            "[[0.25429837 0.25096826 0.25394822 0.24078515]]\n",
            "\n",
            "Je vous dois la vie.\n",
            "[1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Tolstoy\n",
            "\n",
            "Prediction:\n",
            "['Tolstoy']\n",
            "[[0.0466672  0.34852971 0.00792362 0.59687947]]\n",
            "\n",
            "На одной скамье, в уединенной аллее, увидел я m-me M *.\n",
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Dostoevsky\n",
            "\n",
            "Prediction:\n",
            "['Chekhov']\n",
            "[[0.37399333 0.2254538  0.09022878 0.31032409]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "VF43G1cBRyxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to file in the current working directory\n",
        "pkl_filename = \"logreg_latin.pkl\"\n",
        "with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(lr_latin, file)"
      ],
      "metadata": {
        "id": "Y_05LBT2RbjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "gc7R1068SCka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy Model"
      ],
      "metadata": {
        "id": "l4Bdh1NQSG8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_predictions = ['Dostoevsky'] * len(test_sentences)\n",
        "print(len(dummy_predictions))\n",
        "\n",
        "# Calculate the accuracy of these \"dummy predictions\"\n",
        "acc_dummy = accuracy_score(test_authors, dummy_predictions)\n",
        "print(f'The accuracy is: {acc_dummy}')\n",
        "print()\n",
        "\n",
        "print(classification_report(test_authors, dummy_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMH-f8VRR_MS",
        "outputId": "64658e3d-efc2-4371-a672-f0c32a1e2b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "The accuracy is: 0.25\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Chekhov       0.00      0.00      0.00       250\n",
            "  Dostoevsky       0.25      1.00      0.40       250\n",
            "       Gogol       0.00      0.00      0.00       250\n",
            "     Tolstoy       0.00      0.00      0.00       250\n",
            "\n",
            "    accuracy                           0.25      1000\n",
            "   macro avg       0.06      0.25      0.10      1000\n",
            "weighted avg       0.06      0.25      0.10      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latin characters model"
      ],
      "metadata": {
        "id": "ytiBmcFzSO5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy:')\n",
        "\n",
        "acc = accuracy_score(test_authors, test_predictions_latin)\n",
        "print(acc)\n",
        "corr_count = accuracy_score(test_authors, test_predictions_latin, normalize=False)\n",
        "total_count = len(test_authors)\n",
        "\n",
        "print(f'Total reviews: {str(total_count)}')\n",
        "print(f'Total correct predictions: {str(corr_count)}')\n",
        "corr_ratio = corr_count / total_count\n",
        "print(f'Correct ratio: {str(corr_ratio)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql72y9zCSM5z",
        "outputId": "4ff38683-f6d3-40f0-9901-024a120bb0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:\n",
            "0.265\n",
            "Total reviews: 1000\n",
            "Total correct predictions:265\n",
            "Correct ratio:0.265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_authors, test_predictions_latin))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PLxS7M7SoLZ",
        "outputId": "c6f86798-6598-44f8-aea0-64fa4caaa126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Chekhov       0.25      0.99      0.40       250\n",
            "  Dostoevsky       0.50      0.01      0.02       250\n",
            "       Gogol       0.00      0.00      0.00       250\n",
            "     Tolstoy       0.70      0.06      0.12       250\n",
            "\n",
            "    accuracy                           0.27      1000\n",
            "   macro avg       0.36      0.27      0.13      1000\n",
            "weighted avg       0.36      0.27      0.13      1000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}