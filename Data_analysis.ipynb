{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries, downloading the model"
      ],
      "metadata": {
        "id": "Mpj1I_P2K5S4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EIpGQQ5AGrv",
        "outputId": "2a44c5a1-15b6-4321-a667-d17fd78e96b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.3\n",
            "1.2.2\n",
            "1.22.4\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "import sklearn\n",
        "import numpy\n",
        "import spacy\n",
        "import re\n",
        "from statistics import mean\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "print(pandas.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(numpy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_lg\n",
        "nlp = spacy.load('ru_core_news_lg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaM4ef2lSMnQ",
        "outputId": "4feeaca4-e3f4-47cb-9a25-1279d427cbb3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-19 19:41:38.304267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_lg-3.5.0/ru_core_news_lg-3.5.0-py3-none-any.whl (513.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.4/513.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-lg==3.5.0) (3.5.2)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-lg==3.5.0)\n",
            "  Downloading pymorphy3-1.2.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.1.2)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=a18a29e4d624ad27d2db6bd031877c0dc5f43c21514ae8d2e59223a3d2c4e18c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy3-dicts-ru, docopt, dawg-python, pymorphy3, ru-core-news-lg\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy3-1.2.0 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making lists and doc objects from csv files"
      ],
      "metadata": {
        "id": "vRbGoiRNLFHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the training data from a csv file\n",
        "train_set = pandas.read_csv('./train_data.csv', encoding='utf-8')\n",
        "train_set"
      ],
      "metadata": {
        "id": "QSI83sGJAbwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = pandas.read_csv('./test_data.csv', encoding='utf-8')\n",
        "test_set"
      ],
      "metadata": {
        "id": "gFACzHopAgQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get lists of authors and sentences for both the train set and the test set:"
      ],
      "metadata": {
        "id": "5qCa2HaeLC99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_set['text'].to_list()\n",
        "train_authors = train_set['author'].to_list()\n",
        "\n",
        "test_sentences = test_set['text'].to_list()\n",
        "test_authors = test_set['author'].to_list()\n",
        "\n",
        "print(len(train_authors), len(test_authors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px9tzpjhBhG5",
        "outputId": "a00543fa-fb7f-48d8-81c8-99a1a3784d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create doc objects out of the lists above:"
      ],
      "metadata": {
        "id": "yoiRFVIRLUyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "test_doc_sentences = nlp.pipe(test_sentences)"
      ],
      "metadata": {
        "id": "rV1YF9ZcBSMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data per class (author)"
      ],
      "metadata": {
        "id": "cdkHTcOaMSSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It would also be useful to extract the data per author:"
      ],
      "metadata": {
        "id": "9EaW4tjKFAYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_extract(author_name):\n",
        "  needed_data = train_set[train_set['author'] == author_name]['text'].to_list()\n",
        "  return needed_data"
      ],
      "metadata": {
        "id": "1DW3yuPY-Bzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract data for Dostoyevsky\n",
        "dostoyevsky_data = data_extract('Dostoevsky')\n",
        "tolstoy_data = data_extract('Tolstoy')\n",
        "chekhov_data = data_extract('Chekhov')\n",
        "gogol_data = data_extract('Gogol')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBwuY3nHFFZo",
        "outputId": "ac882553-5520-4122-ae9b-35a0cca721c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences by Dostoyevsky: 2500\n",
            "Number of sentences by Tolstoy: 2500\n",
            "Number of sentences by Chekhov: 2500\n",
            "Number of sentences by Gogol: 2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of sentences per author\n",
        "print(f'Number of sentences by Dostoyevsky: {len(dostoyevsky_data)}')\n",
        "print(f'Number of sentences by Tolstoy: {len(tolstoy_data)}')\n",
        "print(f'Number of sentences by Chekhov: {len(chekhov_data)}')\n",
        "print(f'Number of sentences by Gogol: {len(gogol_data)}')"
      ],
      "metadata": {
        "id": "dkO2H7NE-xXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's process the data per each of the four authors with Spacy, thus creating a doc object for list that we made above:"
      ],
      "metadata": {
        "id": "xe81Z62sHqQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dostoyevsky_data_doc = nlp.pipe(dostoyevsky_data)\n",
        "tolstoy_data_doc = nlp.pipe(tolstoy_data)\n",
        "chekhov_data_doc = nlp.pipe(chekhov_data)\n",
        "gogol_data_doc = nlp.pipe(gogol_data)"
      ],
      "metadata": {
        "id": "9bSwnYzsGasH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "E6AwO2S9Ma8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write all the functions necessary for our data analysis:"
      ],
      "metadata": {
        "id": "LRw5_SZ9LeWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_length(sentences):\n",
        "  sentence_lengths = [len(sentence) for sentence in sentences]\n",
        "  avg_sent_length = mean(sentence_lengths)\n",
        "  print(f'Nr of sentences: {len(sentence_lengths)}')\n",
        "  print(f'Average nr of words per sentence: {avg_sent_length}')\n",
        "\n",
        "def get_avg_word_length(sentences):\n",
        "  word_length_list_total = []\n",
        "  for sentence in sentences:\n",
        "    tokens_length_list_per_sentence = [len(token.text) for token in sentence]\n",
        "    avg_length = mean(tokens_length_list_per_sentence)\n",
        "    word_length_list_total.append(avg_length)\n",
        "  avg_word_length = mean(word_length_list_total)\n",
        "  print(f'Nr of sentences: {len(word_length_list_total)}')\n",
        "  print(f'Average token length: {len(avg_word_length)}')\n",
        "\n",
        "def ner_counter(sentences):\n",
        "  all_named_entities = []\n",
        "  for doc in sentences:\n",
        "    for entity in doc.ents:\n",
        "        all_named_entities.append(entity.label_)\n",
        "  named_entity_counts = Counter(all_named_entities)\n",
        "  print(named_entity_counts)\n",
        "\n",
        "def punctuation_mark_counter_greedy(sentences):\n",
        "  new_punctuation_string = \"—«»–‹›…\" + string.punctuation\n",
        "  punctuation_marks = []\n",
        "  for sentence in sentences:\n",
        "    for char in sentence:\n",
        "      if char in new_punctuation_string:\n",
        "        punctuation_marks.append(char)\n",
        "  punctuation_marks_counts = Counter(punctuation_marks)\n",
        "  print(punctuation_marks_counts)\n",
        "\n",
        "def replace_ellipsis(sentences):\n",
        "  updated_sentences = [sentence.replace(\"...\", \"…\") for sentence in sentences]\n",
        "  return updated_sentences\n",
        "\n",
        "def count_sentences_with_latin_chars(sentences):\n",
        "  latin_pattern = '[a-zA-Z]'\n",
        "  count = 0\n",
        "  for sentence in sentences:\n",
        "    if re.search(latin_pattern, sentence):\n",
        "      count += 1\n",
        "      # print(sentence)\n",
        "  print(count)\n",
        "\n",
        "def fivegram_pos_extractor_unique(list_of_doc_sentences):\n",
        "    n = 5\n",
        "    fivegram_pos_tags = []\n",
        "    for doc in list_of_doc_sentences:\n",
        "        # Iterate over each possible fivegram in the document\n",
        "        for i in range(len(doc) - n + 1):\n",
        "            # Extract the tokens for the current fivegram\n",
        "            fivegram_tokens = doc[i : i + n]\n",
        "            # Extract the POS tags of the tokens and add the POS tag combination to the list\n",
        "            fivegram_pos = tuple(token.pos_ for token in fivegram_tokens)\n",
        "            fivegram_pos_tags.append(fivegram_pos)\n",
        "    unique_fivegram_pos_tags = list(set(fivegram_pos_tags))\n",
        "\n",
        "    return unique_fivegram_pos_tags\n",
        "\n",
        "def fivegram_pos_count(list_of_doc_sentences):\n",
        "  n = 5\n",
        "  fivegram_pos_tags = []\n",
        "  for doc in list_of_doc_sentences:\n",
        "    # Iterate over each possible fivegram in the document\n",
        "    for i in range(len(doc) - n + 1):\n",
        "            # Extract the tokens for the current fivegram\n",
        "            fivegram_tokens = doc[i : i + n]\n",
        "            # Extract the POS tags of the tokens and add the POS tag combination to the list\n",
        "            fivegram_pos = tuple(token.pos_ for token in fivegram_tokens)\n",
        "            fivegram_pos_tags.append(fivegram_pos)\n",
        "  most_common_fivegrams = Counter(fivegram_pos_tags).most_common(5)\n",
        "  return most_common_fivegrams"
      ],
      "metadata": {
        "id": "fFwc4M_1HZ_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing the data"
      ],
      "metadata": {
        "id": "Y2Aj41U-LudS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence and word length"
      ],
      "metadata": {
        "id": "neCYnxVOLxuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially I considered using sentence and word length as features for one of the models. However since we had some exercises on it, I thought I might not be allowed to use it and I focused on other features instead."
      ],
      "metadata": {
        "id": "xEovTZM6L4JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentence_length(dostoyevsky_data_doc)\n",
        "get_sentence_length(tolstoy_data_doc)\n",
        "get_sentence_length(chekhov_data_doc)\n",
        "get_sentence_length(gogol_data_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "aAP1CjoAJ3z8",
        "outputId": "9db71302-ae0f-481d-e634-31efbef545ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_sentence_length' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-78341cb813c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_sentence_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdostoyevsky_data_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_sentence_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtolstoy_data_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_sentence_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchekhov_data_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_sentence_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgogol_data_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_sentence_length' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_avg_word_length(dostoyevsky_data_doc)\n",
        "get_avg_word_length(tolstoy_data_doc)\n",
        "get_avg_word_length(chekhov_data_doc)\n",
        "get_avg_word_length(gogol_data_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHUNdMT9Oior",
        "outputId": "1ac61eaf-7fef-4689-8486-69bc79b3f703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nr of sentences: 2500\n",
            "Average token length: 3.7622330273529023\n",
            "Nr of sentences: 2500\n",
            "Average token length: 3.8788479197579444\n",
            "Nr of sentences: 2500\n",
            "Average token length: 3.7922887875675118\n",
            "Nr of sentences: 2500\n",
            "Average token length: 4.026964518720382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Counter"
      ],
      "metadata": {
        "id": "PX9nIRhsMLmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_counter(train_doc_sentences)\n",
        "ner_counter(test_doc_sentences)\n",
        "ner_counter(dostoyevsky_data_doc)\n",
        "ner_counter(tolstoy_data_doc)\n",
        "ner_counter(chekhov_data_doc)\n",
        "ner_counter(gogol_data_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wx4LLcjC5Xq",
        "outputId": "6348bc80-3c52-44cd-aa3b-709ab22fb5d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'PER': 3819, 'LOC': 591, 'ORG': 118})\n",
            "Counter({'PER': 387, 'LOC': 52, 'ORG': 9})\n",
            "Counter({'PER': 819, 'LOC': 98, 'ORG': 25})\n",
            "Counter({'PER': 1196, 'LOC': 236, 'ORG': 55})\n",
            "Counter({'PER': 995, 'LOC': 112, 'ORG': 19})\n",
            "Counter({'PER': 809, 'LOC': 145, 'ORG': 19})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punctuation mark counter"
      ],
      "metadata": {
        "id": "b53P-P6cMVCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting all punctuation marks."
      ],
      "metadata": {
        "id": "uRIHK7pvMaDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation_mark_counter_greedy(train_sentences)\n",
        "punctuation_mark_counter_greedy(test_sentences)\n",
        "punctuation_mark_counter_greedy(dostoyevsky_data)\n",
        "punctuation_mark_counter_greedy(tolstoy_data)\n",
        "punctuation_mark_counter_greedy(chekhov_data)\n",
        "punctuation_mark_counter_greedy(gogol_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlwVZXrgyAi-",
        "outputId": "f244efb0-5366-4da9-9aae-5fd728679c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({',': 21220, '.': 15248, '-': 2112, '!': 1891, '–': 1848, '—': 1820, '?': 1248, ';': 1025, ':': 819, '…': 525, '«': 514, '»': 512, ')': 355, '(': 354, '\"': 239, '[': 98, ']': 98, '&': 98, '#': 97, '*': 95, \"'\": 43, '{': 17, '}': 17, '<': 14, '>': 14, '/': 2, '%': 1})\n",
            "Counter({',': 2101, '.': 1459, '-': 209, '—': 179, '–': 175, '!': 170, '?': 123, ';': 96, ':': 82, '«': 56, '…': 50, '»': 39, ')': 32, '(': 30, '\"': 29, '[': 10, ']': 10, '&': 7, '#': 7, '*': 6, '{': 3, '}': 3, \"'\": 3, '<': 1, '>': 1})\n",
            "Counter({',': 5706, '.': 3261, '–': 838, '-': 727, '!': 427, '?': 368, ';': 298, '…': 191, ':': 190, '«': 165, '»': 161, '\"': 94, '—': 89, '(': 85, ')': 83, '[': 16, ']': 16, '*': 7, \"'\": 2, '<': 1, '>': 1})\n",
            "Counter({',': 5937, '.': 4260, '-': 698, '—': 469, ';': 287, '?': 271, '–': 251, '!': 216, ':': 193, '\"': 112, '«': 111, '»': 110, ')': 105, '(': 101, '…': 99, '&': 97, '#': 97, '[': 66, ']': 65, \"'\": 38, '*': 31, '{': 17, '}': 17, '/': 2, '<': 2, '>': 2})\n",
            "Counter({'.': 5054, ',': 4462, '—': 1027, '!': 659, '-': 335, '?': 324, '…': 143, ':': 136, '–': 128, ')': 107, '(': 106, '»': 105, ';': 103, '«': 100, '*': 54, '&': 1, '%': 1, '<': 1, '>': 1})\n",
            "Counter({',': 5115, '.': 2673, '–': 631, '!': 589, '-': 352, ';': 337, ':': 300, '?': 285, '—': 235, '«': 138, '»': 136, '…': 92, '(': 62, ')': 60, '\"': 33, ']': 17, '[': 16, '<': 10, '>': 10, \"'\": 3, '*': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentences with Latin characters counter"
      ],
      "metadata": {
        "id": "WUSRbcS6MwS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "count_sentences_with_latin_chars(train_sentences)\n",
        "count_sentences_with_latin_chars(test_sentences)\n",
        "count_sentences_with_latin_chars(dostoyevsky_data)\n",
        "count_sentences_with_latin_chars(tolstoy_data)\n",
        "count_sentences_with_latin_chars(chekhov_data)\n",
        "count_sentences_with_latin_chars(gogol_data)"
      ],
      "metadata": {
        "id": "lQw38HP3kdGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9af8ec-a85e-4fd1-922e-9a7ea4a8e32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "277\n",
            "30\n",
            "58\n",
            "163\n",
            "25\n",
            "31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5-gram POS sequence counter"
      ],
      "metadata": {
        "id": "IWTWhu0KM6pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(fivegram_pos_extractor_unique(train_doc_sentences)))\n",
        "print(len(fivegram_pos_extractor_unique(test_doc_sentences)))\n",
        "print(len(fivegram_pos_extractor_unique(dostoyevsky_data_doc)))\n",
        "print(len(fivegram_pos_extractor_unique(tolstoy_data_doc)))\n",
        "print(len(fivegram_pos_extractor_unique(chekhov_data_doc)))\n",
        "print(len(fivegram_pos_extractor_unique(gogol_data_doc)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVOvH_jz-dBz",
        "outputId": "42d0518a-4690-45b8-bfd2-84f85b7b951b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49912\n",
            "10750\n",
            "23180\n",
            "21998\n",
            "17207\n",
            "19514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(fivegram_pos_count(train_doc_sentences))\n",
        "print(fivegram_pos_count(test_doc_sentences))\n",
        "print(fivegram_pos_count(dostoyevsky_data_doc))\n",
        "print(fivegram_pos_count(tolstoy_data_doc))\n",
        "print(fivegram_pos_count(chekhov_data_doc))\n",
        "print(fivegram_pos_count(gogol_data_doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEDqUBah7n0d",
        "outputId": "64cf6fda-f6b3-4988-9d76-a326d729a2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('X', 'X', 'X', 'X', 'X'), 307), (('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), 254), (('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), 248), (('VERB', 'ADP', 'DET', 'NOUN', 'PUNCT'), 210), (('VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), 181)]\n",
            "[(('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), 31), (('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), 26), (('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), 20), (('NOUN', 'PUNCT', 'VERB', 'NOUN', 'PUNCT'), 20), (('VERB', 'ADP', 'NOUN', 'NOUN', 'PUNCT'), 20)]\n",
            "[(('VERB', 'ADP', 'DET', 'NOUN', 'PUNCT'), 60), (('PUNCT', 'SPACE', 'PUNCT', 'VERB', 'PRON'), 59), (('NOUN', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), 56), (('VERB', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), 42), (('VERB', 'PRON', 'ADP', 'NOUN', 'PUNCT'), 40)]\n",
            "[(('X', 'X', 'X', 'X', 'X'), 297), (('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), 87), (('X', 'X', 'X', 'X', 'PUNCT'), 87), (('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), 84), (('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), 59)]\n",
            "[(('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), 62), (('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), 60), (('PUNCT', 'PUNCT', 'VERB', 'PRON', 'PUNCT'), 49), (('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT'), 48), (('ADJ', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT'), 47)]\n",
            "[(('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), 79), (('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), 68), (('VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), 64), (('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), 59), (('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT'), 52)]\n"
          ]
        }
      ]
    }
  ]
}