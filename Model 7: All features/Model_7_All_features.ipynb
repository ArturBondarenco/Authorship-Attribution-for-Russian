{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data"
      ],
      "metadata": {
        "id": "5yJ_aq8v1eCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries, downloading the model"
      ],
      "metadata": {
        "id": "Nggf7REv1h7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-MRDAun0_Wz",
        "outputId": "836ca0b6-4b57-45bd-e1b4-1e9239f50cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.3\n",
            "1.2.2\n",
            "1.22.4\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "import sklearn\n",
        "import numpy\n",
        "\n",
        "print(pandas.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(numpy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Small Russian model:\n",
        "# !python -m spacy download ru_core_news_sm\n",
        "# nlp = spacy.load('ru_core_news_sm')\n",
        "\n",
        "# Large Russian model:\n",
        "!python -m spacy download ru_core_news_lg\n",
        "nlp = spacy.load('ru_core_news_lg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGeB8Ctv1NHg",
        "outputId": "7529c0c4-d69c-402b-f4a7-cfe5399ee0b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-19 18:06:02.843617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_lg-3.5.0/ru_core_news_lg-3.5.0-py3-none-any.whl (513.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.4/513.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-lg==3.5.0) (3.5.2)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-lg==3.5.0)\n",
            "  Downloading pymorphy3-1.2.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.1.2)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=985f839c47376b42d048bfa1e463ab58197d4cd9e6d108e2e06818ee42df3407\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy3-dicts-ru, docopt, dawg-python, pymorphy3, ru-core-news-lg\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy3-1.2.0 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making lists and doc objects from csv files"
      ],
      "metadata": {
        "id": "ficI2u7k1syz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the training data from a csv file\n",
        "train_set = pandas.read_csv('./train_data.csv', encoding='utf-8')\n",
        "# train_set"
      ],
      "metadata": {
        "id": "SOmVYzPw1ZZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = pandas.read_csv('./test_data.csv', encoding='utf-8')\n",
        "# test_set"
      ],
      "metadata": {
        "id": "CYMRFGDJ2JNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_set['text'].to_list()\n",
        "train_authors = train_set['author'].to_list()\n",
        "\n",
        "test_sentences = test_set['text'].to_list()\n",
        "test_authors = test_set['author'].to_list()\n",
        "\n",
        "print(len(train_authors), len(test_authors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcKpnyJa2M7x",
        "outputId": "91b99257-c4ca-4218-89a3-633d958940e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing the text, replacing \"...\" with \"…\""
      ],
      "metadata": {
        "id": "Rtg3p5VP2z0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_ellipsis(sentences):\n",
        "  updated_sentences = []\n",
        "  for sentence in sentences:\n",
        "    updated_sentence = sentence.replace(\"...\", \"…\")\n",
        "    updated_sentences.append(updated_sentence)\n",
        "  return updated_sentences"
      ],
      "metadata": {
        "id": "LRD3WdXj2451"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dg6KPFVl2QyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract data for Dostoyevsky\n",
        "dostoyevsky_data = train_set[train_set['author'] == 'Dostoevsky']['text'].to_list()\n",
        "\n",
        "# Extract data for Tolstoy\n",
        "tolstoy_data = train_set[train_set['author'] == 'Tolstoy']['text'].to_list()\n",
        "\n",
        "# Extract data for Chekhov\n",
        "chekhov_data = train_set[train_set['author'] == 'Chekhov']['text'].to_list()\n",
        "\n",
        "# Extract data for Gogol\n",
        "gogol_data = train_set[train_set['author'] == 'Gogol']['text'].to_list()\n",
        "\n",
        "train_sentences = replace_ellipsis(train_sentences)\n",
        "test_sentences = replace_ellipsis(test_sentences)\n",
        "\n",
        "dostoyevsky_data = replace_ellipsis(dostoyevsky_data)\n",
        "tolstoy_data = replace_ellipsis(tolstoy_data)\n",
        "chekhov_data = replace_ellipsis(chekhov_data)\n",
        "gogol_data = replace_ellipsis(gogol_data)"
      ],
      "metadata": {
        "id": "8J8DaLBa2cYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "test_doc_sentences = nlp.pipe(test_sentences)\n",
        "\n",
        "dostoyevsky_data_doc = nlp.pipe(dostoyevsky_data)\n",
        "tolstoy_data_doc = nlp.pipe(tolstoy_data)\n",
        "chekhov_data_doc = nlp.pipe(chekhov_data)\n",
        "gogol_data_doc = nlp.pipe(gogol_data)"
      ],
      "metadata": {
        "id": "eKKqguGn3UTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the feature set"
      ],
      "metadata": {
        "id": "yAOJ9ZoF2fVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def fivegram_pos_count(list_of_doc_sentences):\n",
        "  n = 5\n",
        "  fivegram_pos_tags = []\n",
        "  for doc in list_of_doc_sentences:\n",
        "    # Iterate over each possible fivegram in the document\n",
        "    for i in range(len(doc) - n + 1):\n",
        "            # Extract the tokens for the current fivegram\n",
        "            fivegram_tokens = doc[i : i + n]\n",
        "            # Extract the POS tags of the tokens and add the POS tag combination to the list\n",
        "            fivegram_pos = tuple(token.pos_ for token in fivegram_tokens)\n",
        "            fivegram_pos_tags.append(fivegram_pos)\n",
        "  most_common_fivegrams = Counter(fivegram_pos_tags).most_common(5)\n",
        "  five_fivegrams_list = [element[0] for element in most_common_fivegrams]\n",
        "  return five_fivegrams_list\n",
        "\n",
        "def fivegram_pos_extractor_from_sentence(doc):\n",
        "    n = 5\n",
        "    fivegram_pos_tags = []\n",
        "    for i in range(len(doc) - n + 1):\n",
        "    # Extract the tokens for the current fivegram\n",
        "        fivegram_tokens = doc[i : i + n]\n",
        "        # Extract the POS tags of the tokens and add the POS tag combination to the list\n",
        "        fivegram_pos = tuple(token.pos_ for token in fivegram_tokens)\n",
        "        fivegram_pos_tags.append(fivegram_pos)\n",
        "    unique_fivegram_pos_tags = list(set(fivegram_pos_tags))\n",
        "\n",
        "    return unique_fivegram_pos_tags"
      ],
      "metadata": {
        "id": "HJstAXpY2nwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "five_fivegrams_dostoyevsky = fivegram_pos_count(dostoyevsky_data_doc)\n",
        "five_fivegrams_tolstoy = fivegram_pos_count(tolstoy_data_doc)\n",
        "five_fivegrams_chekhov = fivegram_pos_count(chekhov_data_doc)\n",
        "five_fivegrams_gogol = fivegram_pos_count(gogol_data_doc)\n",
        "\n",
        "fivegrams_list = five_fivegrams_dostoyevsky + five_fivegrams_tolstoy + five_fivegrams_chekhov + five_fivegrams_gogol\n",
        "fivegrams_list = list(set(fivegrams_list))"
      ],
      "metadata": {
        "id": "tT2KdjKr3YJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(five_fivegrams_dostoyevsky)\n",
        "print(five_fivegrams_tolstoy)\n",
        "print(five_fivegrams_chekhov)\n",
        "print(five_fivegrams_gogol)\n",
        "print(len(fivegrams_list), fivegrams_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FKBH75P7ODb",
        "outputId": "784cd510-2b57-4415-b11b-7819440348cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('VERB', 'ADP', 'DET', 'NOUN', 'PUNCT'), ('PUNCT', 'SPACE', 'PUNCT', 'VERB', 'PRON'), ('NOUN', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('VERB', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('VERB', 'PRON', 'ADP', 'NOUN', 'PUNCT')]\n",
            "[('X', 'X', 'X', 'X', 'X'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('X', 'X', 'X', 'X', 'PUNCT'), ('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT')]\n",
            "[('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('PUNCT', 'PUNCT', 'VERB', 'PRON', 'PUNCT'), ('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('ADJ', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT')]\n",
            "[('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT')]\n",
            "14 [('VERB', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('VERB', 'PRON', 'ADP', 'NOUN', 'PUNCT'), ('X', 'X', 'X', 'X', 'X'), ('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('ADJ', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT'), ('X', 'X', 'X', 'X', 'PUNCT'), ('PUNCT', 'PUNCT', 'VERB', 'PRON', 'PUNCT'), ('PUNCT', 'SPACE', 'PUNCT', 'VERB', 'PRON'), ('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('VERB', 'ADP', 'DET', 'NOUN', 'PUNCT'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "new_punctuation_string = \"—«»–‹›…\" + string.punctuation\n",
        "print(new_punctuation_string)\n",
        "print(len(new_punctuation_string))\n",
        "\n",
        "french_accent_marks = \"éèêëàÉÈÊËÀ\"\n",
        "new_string = string.ascii_letters + french_accent_marks\n",
        "print(len(new_string), new_string)\n",
        "\n",
        "final_string = new_punctuation_string + new_string\n",
        "print(len(final_string), final_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nDoCn1J7mm7",
        "outputId": "5e81f723-d82e-490c-fa36-e60ceb6843fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "—«»–‹›…!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "39\n",
            "62 abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZéèêëàÉÈÊËÀ\n",
            "101 —«»–‹›…!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZéèêëàÉÈÊËÀ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "entity_types = ['PER', 'LOC', 'ORG']\n",
        "\n",
        "feature_nr = 3 + len(fivegrams_list) + len(final_string)\n",
        "print(feature_nr)\n",
        "\n",
        "# We are creating a matrix with zero vectors for each review (in training set and test set)\n",
        "train_features_matrix = numpy.zeros((len(train_sentences), feature_nr))\n",
        "print(train_features_matrix.shape)\n",
        "\n",
        "test_features_matrix = numpy.zeros((len(test_sentences), feature_nr))\n",
        "print(test_features_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lZrjVsR7uAd",
        "outputId": "8af85a69-cf83-4f98-e88b-dd55b938a7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118\n",
            "(10000, 118)\n",
            "(1000, 118)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying the feature vectors"
      ],
      "metadata": {
        "id": "ejqz5d_x8JIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation tests"
      ],
      "metadata": {
        "id": "VNyza7Au8O2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for sentence, author in zip(train_doc_sentences, train_authors):\n",
        "    print(author)\n",
        "    print(sentence)\n",
        "    NEs_in_sentence = [entity.label_ for entity in sentence.ents]\n",
        "    print(NEs_in_sentence)\n",
        "\n",
        "#NER:\n",
        "    for entity_type in entity_types:\n",
        "      if entity_type in NEs_in_sentence:\n",
        "        print(entity_type)\n",
        "        entity_id = entity_types.index(entity_type)\n",
        "        print(entity_id)\n",
        "    fivegram_pos_list = fivegram_pos_extractor_from_sentence(sentence)\n",
        "    print(fivegram_pos_list)\n",
        "\n",
        "#Common 5-gram POS:\n",
        "    for fivegram in fivegrams_list:\n",
        "      if fivegram in fivegram_pos_list:\n",
        "        print(fivegram)\n",
        "        fivegram_id = fivegrams_list.index(fivegram) + 3\n",
        "        print(fivegram_id)\n",
        "\n",
        "#Punctuation + Latin characters:\n",
        "      for char in sentence.text:\n",
        "        if char in final_string:\n",
        "          print(char)\n",
        "          char_id = final_string.index(char) + 3 + len(fivegrams_list)\n",
        "          print(char_id)\n",
        "\n",
        "    print()\n",
        "    counter +=1\n",
        "    if counter == 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "m-umTVe08EB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation test 2:"
      ],
      "metadata": {
        "id": "maewZeN1-q0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process a text\n",
        "train_features_matrix = numpy.zeros((len(train_sentences), feature_nr))\n",
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "\n",
        "counter = 0\n",
        "# loop over each review, label and feature vector at the same time (zip)\n",
        "for sentence, author, feature_vector in zip(train_doc_sentences, train_authors, train_features_matrix):\n",
        "    print('Author:', author)\n",
        "    print(sentence)\n",
        "    NEs_in_sentence = [entity.label_ for entity in sentence.ents]\n",
        "\n",
        "#NER:\n",
        "    for entity_type in entity_types:\n",
        "      if entity_type in NEs_in_sentence:\n",
        "        entity_id = entity_types.index(entity_type)\n",
        "        print(entity_type)\n",
        "        print(entity_id)\n",
        "        feature_vector[entity_id] = 1\n",
        "        print(feature_vector)\n",
        "    fivegram_pos_list = fivegram_pos_extractor_from_sentence(sentence)\n",
        "    print(fivegram_pos_list)\n",
        "\n",
        "#Common 5-gram POS:\n",
        "    for fivegram in fivegrams_list:\n",
        "      if fivegram in fivegram_pos_list:\n",
        "        fivegram_id = fivegrams_list.index(fivegram) + 3\n",
        "        print(fivegram_id)\n",
        "        print(fivegram)\n",
        "        feature_vector[fivegram_id] = 1\n",
        "        print(feature_vector)\n",
        "\n",
        "#Punctuation + Latin characters:\n",
        "    for char in sentence.text:\n",
        "      if char in final_string:\n",
        "        print(char)\n",
        "        char_id = final_string.index(char) + 3 + len(fivegrams_list)\n",
        "        feature_vector[char_id] = 1\n",
        "        print(char_id)\n",
        "        print(feature_vector)\n",
        "\n",
        "    print()\n",
        "    counter +=1\n",
        "    if counter == 2:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDjxMPXI8-Ye",
        "outputId": "0853d963-7d3b-49c7-b2c7-0cb8d3764cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Dostoevsky\n",
            "Но каково же было мое изумление, когда Наташа с первых же слов остановила меня и сказала, что нечего ее утешать, что она уже пять дней, как знает про это..     – Боже мой!\n",
            "PER\n",
            "0\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[('ADV', 'NUM', 'NOUN', 'PUNCT', 'SCONJ'), ('PUNCT', 'SCONJ', 'PROPN', 'ADP', 'ADJ'), ('ADP', 'ADJ', 'PART', 'NOUN', 'VERB'), ('SPACE', 'PUNCT', 'NOUN', 'DET', 'PUNCT'), ('NUM', 'NOUN', 'PUNCT', 'SCONJ', 'VERB'), ('ADJ', 'PART', 'AUX', 'DET', 'NOUN'), ('PRON', 'PUNCT', 'SPACE', 'PUNCT', 'NOUN'), ('NOUN', 'PUNCT', 'SCONJ', 'VERB', 'ADP'), ('PUNCT', 'SCONJ', 'VERB', 'ADP', 'PRON'), ('SCONJ', 'PROPN', 'ADP', 'ADJ', 'PART'), ('PUNCT', 'SCONJ', 'PRON', 'ADV', 'NUM'), ('PUNCT', 'SCONJ', 'VERB', 'PRON', 'VERB'), ('SCONJ', 'VERB', 'ADP', 'PRON', 'PUNCT'), ('ADP', 'PRON', 'PUNCT', 'SPACE', 'PUNCT'), ('SCONJ', 'VERB', 'PRON', 'VERB', 'PUNCT'), ('ADJ', 'PART', 'NOUN', 'VERB', 'PRON'), ('PART', 'AUX', 'DET', 'NOUN', 'PUNCT'), ('VERB', 'PUNCT', 'SCONJ', 'PRON', 'ADV'), ('SCONJ', 'PRON', 'ADV', 'NUM', 'NOUN'), ('PROPN', 'ADP', 'ADJ', 'PART', 'NOUN'), ('PUNCT', 'SPACE', 'PUNCT', 'NOUN', 'DET'), ('VERB', 'PRON', 'CCONJ', 'VERB', 'PUNCT'), ('AUX', 'DET', 'NOUN', 'PUNCT', 'SCONJ'), ('NOUN', 'VERB', 'PRON', 'CCONJ', 'VERB'), ('VERB', 'PRON', 'VERB', 'PUNCT', 'SCONJ'), ('PRON', 'CCONJ', 'VERB', 'PUNCT', 'SCONJ'), ('PRON', 'ADV', 'NUM', 'NOUN', 'PUNCT'), ('CCONJ', 'VERB', 'PUNCT', 'SCONJ', 'VERB'), ('VERB', 'ADP', 'PRON', 'PUNCT', 'SPACE'), ('NOUN', 'PUNCT', 'SCONJ', 'PROPN', 'ADP'), ('VERB', 'PUNCT', 'SCONJ', 'VERB', 'PRON'), ('PRON', 'VERB', 'PUNCT', 'SCONJ', 'PRON'), ('PART', 'NOUN', 'VERB', 'PRON', 'CCONJ'), ('CCONJ', 'ADJ', 'PART', 'AUX', 'DET'), ('DET', 'NOUN', 'PUNCT', 'SCONJ', 'PROPN')]\n",
            ",\n",
            "35\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            ",\n",
            "35\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            ",\n",
            "35\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            ",\n",
            "35\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            ".\n",
            "37\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            ".\n",
            "37\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "–\n",
            "20\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "!\n",
            "24\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Author: Gogol\n",
            "— закричали в толпе.. — Давай совет кошевой!\n",
            "[('PUNCT', 'PUNCT', 'VERB', 'NOUN', 'PROPN'), ('PUNCT', 'VERB', 'NOUN', 'PROPN', 'PUNCT'), ('ADP', 'NOUN', 'PUNCT', 'PUNCT', 'VERB'), ('NOUN', 'PUNCT', 'PUNCT', 'VERB', 'NOUN'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('VERB', 'ADP', 'NOUN', 'PUNCT', 'PUNCT')]\n",
            "15\n",
            "('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT')\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "—\n",
            "17\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            ".\n",
            "37\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            ".\n",
            "37\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "—\n",
            "17\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "!\n",
            "24\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing a function for feature vector modification"
      ],
      "metadata": {
        "id": "NtvO_12N_7gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_feature_vectors(doc_sentences, features_matrix):\n",
        "  for sentence, feature_vector in zip(doc_sentences, features_matrix):\n",
        "    NEs_in_sentence = [entity.label_ for entity in sentence.ents]\n",
        "\n",
        "#NER:\n",
        "    for entity_type in entity_types:\n",
        "      if entity_type in NEs_in_sentence:\n",
        "        entity_id = entity_types.index(entity_type)\n",
        "        feature_vector[entity_id] = 1\n",
        "\n",
        "#Common 5-gram POS:\n",
        "    fivegram_pos_list = fivegram_pos_extractor_from_sentence(sentence)\n",
        "    for fivegram in fivegrams_list:\n",
        "      if fivegram in fivegram_pos_list:\n",
        "        fivegram_id = fivegrams_list.index(fivegram) + 3\n",
        "        feature_vector[fivegram_id] = 1\n",
        "\n",
        "#Punctuation + Latin characters:\n",
        "    for char in sentence.text:\n",
        "      if char in final_string:\n",
        "        char_id = final_string.index(char) + 3 + len(fivegrams_list)\n",
        "        feature_vector[char_id] = 1\n",
        "\n",
        "  return features_matrix"
      ],
      "metadata": {
        "id": "etbJstM-_b6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_matrix = numpy.zeros((len(train_sentences), feature_nr))\n",
        "train_doc_sentences = nlp.pipe(train_sentences)"
      ],
      "metadata": {
        "id": "sQPWmAzXBeBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_matrix_final = modify_feature_vectors(train_doc_sentences, train_features_matrix)"
      ],
      "metadata": {
        "id": "Uwysho2jBsMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "9FtOJxmrCKxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr8_all_features = LogisticRegression()\n",
        "\n",
        "# Train the model on the data, storing the information learned from the dat`a\n",
        "# Model is learning the relationship between digits (x_train) and labels (y_train)\n",
        "lr8_all_features.fit(train_features_matrix_final, train_authors)\n",
        "\n",
        "print(lr8_all_features.classes_)\n",
        "print(lr8_all_features.get_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYa8kNskBuWa",
        "outputId": "1f664b7c-0cfd-46bf-a247-eedec101f023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chekhov' 'Dostoevsky' 'Gogol' 'Tolstoy']\n",
            "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying the test set feature vectors"
      ],
      "metadata": {
        "id": "DpJBPg2rCQnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_doc_sentences = nlp.pipe(test_sentences)\n",
        "test_features_matrix = numpy.zeros((len(test_sentences), feature_nr))\n",
        "\n",
        "test_features_matrix_final = modify_feature_vectors(test_doc_sentences, test_features_matrix)"
      ],
      "metadata": {
        "id": "LtcyUEwZCHN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making predictions"
      ],
      "metadata": {
        "id": "QvP44esbCkBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(i):\n",
        "    print(test_sentences[i])\n",
        "    # print the features of the index\n",
        "    print(test_features_matrix_final[i])\n",
        "    # print the correct label of the index\n",
        "    print(test_authors[i])\n",
        "\n",
        "    print()\n",
        "    print(\"Prediction:\")\n",
        "    # print the prediction for the features of this index\n",
        "    print(lr8_all_features.predict([test_features_matrix_final[i]]))\n",
        "    # print the probabilities for each label predictions\n",
        "    print(lr8_all_features.predict_proba([test_features_matrix_final[i]]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "eyYX0u7KCiOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(0)\n",
        "predict(1)\n",
        "predict(2)\n",
        "predict(3)\n",
        "predict(4)\n",
        "predict(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XryS-uIXCqpI",
        "outputId": "89fefd80-dee5-4e04-f005-01e5c110232c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Фома Фомич, говорю, разве это возможное дело?\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Dostoevsky\n",
            "\n",
            "Prediction:\n",
            "['Tolstoy']\n",
            "[[0.01471629 0.3816194  0.12940678 0.47425753]]\n",
            "\n",
            "Пора бы уже домой.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Chekhov\n",
            "\n",
            "Prediction:\n",
            "['Tolstoy']\n",
            "[[0.23393698 0.23732886 0.25183665 0.27689751]]\n",
            "\n",
            "А казаки все до одного прощались, зная, что много будет работы тем и другим, но не повершили, однако ж, тотчас разлучиться, а повершили дождаться темной ночной поры, чтоб не дать неприятелю увидеть убыль в казацком войске.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Gogol\n",
            "\n",
            "Prediction:\n",
            "['Gogol']\n",
            "[[0.16700823 0.17027571 0.35064359 0.31207246]]\n",
            "\n",
            "Вдруг слезы градом у обоих из глаз, дрогнули руки.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Dostoevsky\n",
            "\n",
            "Prediction:\n",
            "['Dostoevsky']\n",
            "[[0.19172175 0.29075332 0.24826501 0.26925993]]\n",
            "\n",
            "Но художник видел в этом нежном личике одну только заманчивую для кисти почти фарфоровую проэрачность тела, увлекательную легкую томность, тонкую светлую шейку и аристократическую легкостъ стана.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Gogol\n",
            "\n",
            "Prediction:\n",
            "['Dostoevsky']\n",
            "[[0.19172175 0.29075332 0.24826501 0.26925993]]\n",
            "\n",
            "Когда я в Берлине получил оттуда несколько маленьких писем, которые они уже успели мне написать, то тут только я и понял, как их любил.\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Dostoevsky\n",
            "\n",
            "Prediction:\n",
            "['Tolstoy']\n",
            "[[0.16027081 0.19932861 0.27261121 0.36778936]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions_all_features = lr8_all_features.predict(test_features_matrix_final)\n",
        "\n",
        "for p, r in zip(test_predictions_all_features[:10], test_authors[:10]):\n",
        "    if p == r:\n",
        "        result = \"Correct\"\n",
        "    else:\n",
        "        result = \"Incorrect\"\n",
        "    print(p + \"(\" + result + \":\" + r + \")\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0QWE41UCsIn",
        "outputId": "27245005-c018-48a8-f84a-50db7f38fa6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tolstoy(Incorrect:Dostoevsky)\n",
            "Tolstoy(Incorrect:Chekhov)\n",
            "Gogol(Correct:Gogol)\n",
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Dostoevsky(Incorrect:Gogol)\n",
            "Tolstoy(Incorrect:Dostoevsky)\n",
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Chekhov(Correct:Chekhov)\n",
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Gogol(Incorrect:Dostoevsky)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "p7Abq8CADBIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save to file in the current working directory\n",
        "pkl_filename = \"logreg8_all_features.pkl\"\n",
        "with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(lr8_all_features, file)"
      ],
      "metadata": {
        "id": "O1CZvQr9C-1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "PLz4qLN5DSmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy model"
      ],
      "metadata": {
        "id": "b8qxxJXODUTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "dummy_predictions = ['Dostoevsky'] * len(test_sentences)\n",
        "print(len(dummy_predictions))\n",
        "\n",
        "# Calculate the accuracy of these \"dummy predictions\"\n",
        "acc_dummy = accuracy_score(test_authors, dummy_predictions)\n",
        "print('The accuracy is:', acc_dummy)\n",
        "print()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_authors, dummy_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whpLL3MBDJno",
        "outputId": "0dad2fcf-ecf7-47f7-f048-654bfc8b0583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "The accuracy is: 0.25\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Chekhov       0.00      0.00      0.00       250\n",
            "  Dostoevsky       0.25      1.00      0.40       250\n",
            "       Gogol       0.00      0.00      0.00       250\n",
            "     Tolstoy       0.00      0.00      0.00       250\n",
            "\n",
            "    accuracy                           0.25      1000\n",
            "   macro avg       0.06      0.25      0.10      1000\n",
            "weighted avg       0.06      0.25      0.10      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER + Common POS Model"
      ],
      "metadata": {
        "id": "wfwxSCRJDb4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Accuracy:')\n",
        "\n",
        "acc = accuracy_score(test_authors, test_predictions_all_features)\n",
        "print(acc)\n",
        "corr_count = accuracy_score(test_authors, test_predictions_all_features, normalize=False)\n",
        "total_count = len(test_authors)\n",
        "\n",
        "print(\"Total reviews: \" + str(str(total_count)))\n",
        "print(\"Total correct predictions:\" + str(corr_count))\n",
        "corr_ratio = corr_count / total_count\n",
        "print(\"Correct ratio:\" + str(corr_ratio))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e7V4qlhDavo",
        "outputId": "dcbcf575-e177-49a9-a051-d3324df3e400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:\n",
            "0.394\n",
            "Total reviews: 1000\n",
            "Total correct predictions:394\n",
            "Correct ratio:0.394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_authors, test_predictions_all_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bkrd4k_DoTy",
        "outputId": "160ea577-1d3b-4fc3-fb29-653685c17eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Chekhov       0.55      0.43      0.48       250\n",
            "  Dostoevsky       0.32      0.38      0.35       250\n",
            "       Gogol       0.42      0.34      0.38       250\n",
            "     Tolstoy       0.34      0.43      0.38       250\n",
            "\n",
            "    accuracy                           0.39      1000\n",
            "   macro avg       0.41      0.39      0.40      1000\n",
            "weighted avg       0.41      0.39      0.40      1000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}