{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data"
      ],
      "metadata": {
        "id": "UH33eq9vsjbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries, downloading the model"
      ],
      "metadata": {
        "id": "Ll1FX6bIsp2Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwUIC2VhsOgq",
        "outputId": "99e89d44-d592-4e08-86a0-496a9a532629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.3\n",
            "1.2.2\n",
            "1.22.4\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "import sklearn\n",
        "import numpy\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import sys\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(pandas.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(numpy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small Russian model:\n",
        "# !python -m spacy download ru_core_news_sm\n",
        "# nlp = spacy.load('ru_core_news_sm')\n",
        "\n",
        "# Large Russian model:\n",
        "!python -m spacy download ru_core_news_lg\n",
        "nlp = spacy.load('ru_core_news_lg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO5wMjnTswS7",
        "outputId": "48480025-7c04-4bb6-ed25-50f0f7af9694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-19 17:28:37.084662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_lg-3.5.0/ru_core_news_lg-3.5.0-py3-none-any.whl (513.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.4/513.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-lg==3.5.0) (3.5.2)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-lg==3.5.0)\n",
            "  Downloading pymorphy3-1.2.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.1.2)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=de0ec6a23ad54b12fdb1bee0359519b926485c226ccd644a119235a8546e2363\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy3-dicts-ru, docopt, dawg-python, pymorphy3, ru-core-news-lg\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy3-1.2.0 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making lists and doc objects from csv files"
      ],
      "metadata": {
        "id": "q6JG9oWjs950"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the training data from a csv file\n",
        "train_set = pandas.read_csv('./train_data.csv', encoding='utf-8')\n",
        "# train_set"
      ],
      "metadata": {
        "id": "oPaYcgf4s1Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = pandas.read_csv('./test_data.csv', encoding='utf-8')\n",
        "# test_set"
      ],
      "metadata": {
        "id": "2njbKY6ttCpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_set['text'].to_list()\n",
        "train_authors = train_set['author'].to_list()\n",
        "\n",
        "test_sentences = test_set['text'].to_list()\n",
        "test_authors = test_set['author'].to_list()\n",
        "\n",
        "print(len(train_authors), len(test_authors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Is_APxitErJ",
        "outputId": "9b00ea58-e2c6-4199-a15a-6fe3cdd2dc16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "test_doc_sentences = nlp.pipe(test_sentences)"
      ],
      "metadata": {
        "id": "ZL2EA3d6tI9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the feature set"
      ],
      "metadata": {
        "id": "KxT-WNFitv28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract data for Dostoyevsky\n",
        "dostoyevsky_data = train_set[train_set['author'] == 'Dostoevsky']['text'].to_list()\n",
        "\n",
        "# Extract data for Tolstoy\n",
        "tolstoy_data = train_set[train_set['author'] == 'Tolstoy']['text'].to_list()\n",
        "\n",
        "# Extract data for Chekhov\n",
        "chekhov_data = train_set[train_set['author'] == 'Chekhov']['text'].to_list()\n",
        "\n",
        "# Extract data for Gogol\n",
        "gogol_data = train_set[train_set['author'] == 'Gogol']['text'].to_list()\n",
        "\n",
        "dostoyevsky_data_doc = nlp.pipe(dostoyevsky_data)\n",
        "tolstoy_data_doc = nlp.pipe(tolstoy_data)\n",
        "chekhov_data_doc = nlp.pipe(chekhov_data)\n",
        "gogol_data_doc = nlp.pipe(gogol_data)"
      ],
      "metadata": {
        "id": "xSx6OLn8tLSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fivegram_pos_count(list_of_doc_sentences):\n",
        "  n = 5\n",
        "  fivegram_pos_tags = []\n",
        "  for doc in list_of_doc_sentences:\n",
        "    # Iterate over each possible fivegram in the document\n",
        "    for i in range(len(doc) - n + 1):\n",
        "            # Extract the tokens for the current fivegram\n",
        "            fivegram_tokens = doc[i : i + n]\n",
        "            # Extract the POS tags of the tokens and add the POS tag combination to the list\n",
        "            fivegram_pos = tuple(token.pos_ for token in fivegram_tokens)\n",
        "            fivegram_pos_tags.append(fivegram_pos)\n",
        "  most_common_fivegrams = Counter(fivegram_pos_tags).most_common(5)\n",
        "  five_fivegrams_list = [element[0] for element in most_common_fivegrams]\n",
        "  return five_fivegrams_list\n",
        "\n",
        "def fivegram_pos_extractor_from_sentence(doc):\n",
        "    n = 5\n",
        "    fivegram_pos_tags = []\n",
        "    for i in range(len(doc) - n + 1):\n",
        "    # Extract the tokens for the current fivegram\n",
        "        fivegram_tokens = doc[i : i + n]\n",
        "        # Extract the POS tags of the tokens and add the POS tag combination to the list\n",
        "        fivegram_pos = tuple(token.pos_ for token in fivegram_tokens)\n",
        "        fivegram_pos_tags.append(fivegram_pos)\n",
        "    unique_fivegram_pos_tags = list(set(fivegram_pos_tags))\n",
        "\n",
        "    return unique_fivegram_pos_tags"
      ],
      "metadata": {
        "id": "hct7VVBvtqX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "five_fivegrams_dostoyevsky = fivegram_pos_count(dostoyevsky_data_doc)\n",
        "five_fivegrams_tolstoy = fivegram_pos_count(tolstoy_data_doc)\n",
        "five_fivegrams_chekhov = fivegram_pos_count(chekhov_data_doc)\n",
        "five_fivegrams_gogol = fivegram_pos_count(gogol_data_doc)\n",
        "\n",
        "fivegrams_list = five_fivegrams_dostoyevsky + five_fivegrams_tolstoy + five_fivegrams_chekhov + five_fivegrams_gogol\n",
        "fivegrams_list = list(set(fivegrams_list))"
      ],
      "metadata": {
        "id": "884CGLOuuL86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(five_fivegrams_dostoyevsky)\n",
        "print(five_fivegrams_tolstoy)\n",
        "print(five_fivegrams_chekhov)\n",
        "print(five_fivegrams_gogol)\n",
        "print(len(fivegrams_list), fivegrams_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVxe8TGSuMvX",
        "outputId": "29e85c58-aa44-4d98-8d5d-86ac5094c977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('VERB', 'ADP', 'DET', 'NOUN', 'PUNCT'), ('PUNCT', 'SPACE', 'PUNCT', 'VERB', 'PRON'), ('NOUN', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('VERB', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('VERB', 'PRON', 'ADP', 'NOUN', 'PUNCT')]\n",
            "[('X', 'X', 'X', 'X', 'X'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('X', 'X', 'X', 'X', 'PUNCT'), ('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT')]\n",
            "[('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('PUNCT', 'PUNCT', 'VERB', 'PRON', 'PUNCT'), ('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('ADJ', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT')]\n",
            "[('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT')]\n",
            "14 [('ADJ', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('NOUN', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('VERB', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('PUNCT', 'SPACE', 'PUNCT', 'VERB', 'PRON'), ('VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('VERB', 'ADP', 'DET', 'NOUN', 'PUNCT'), ('PUNCT', 'PUNCT', 'VERB', 'PRON', 'PUNCT'), ('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('VERB', 'PRON', 'ADP', 'NOUN', 'PUNCT'), ('X', 'X', 'X', 'X', 'PUNCT'), ('X', 'X', 'X', 'X', 'X'), ('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "entity_types = ['PER', 'LOC', 'ORG']\n",
        "\n",
        "# We are creating a matrix with zero vectors for each review (in training set and test set)\n",
        "train_features_matrix = numpy.zeros((len(train_sentences), 3 + len(fivegrams_list)))\n",
        "print(train_features_matrix.shape)\n",
        "\n",
        "test_features_matrix = numpy.zeros((len(test_sentences), 3 + len(fivegrams_list)))\n",
        "print(test_features_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJZrmBiruP_A",
        "outputId": "4ddc9306-a877-42d2-9977-dd26c88b6f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 17)\n",
            "(1000, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying the feature vectors"
      ],
      "metadata": {
        "id": "y7rsBSy6uteo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation tests"
      ],
      "metadata": {
        "id": "ig7m2us2uzRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for sentence, author in zip(train_doc_sentences, train_authors):\n",
        "    print(author)\n",
        "    print(sentence)\n",
        "    NEs_in_sentence = [entity.label_ for entity in sentence.ents]\n",
        "    print(NEs_in_sentence)\n",
        "    for entity_type in entity_types:\n",
        "      if entity_type in NEs_in_sentence:\n",
        "        print(entity_type)\n",
        "    fivegram_pos_list = fivegram_pos_extractor_from_sentence(sentence)\n",
        "    print(fivegram_pos_list)\n",
        "    for fivegram in fivegrams_list:\n",
        "      if fivegram in fivegram_pos_list:\n",
        "        print(fivegram)\n",
        "        fivegram_id = fivegrams_list.index(fivegram)\n",
        "        print(fivegram_id)\n",
        "    print()\n",
        "    counter +=1\n",
        "    if counter == 5:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6C-0P__unAm",
        "outputId": "beaa3db1-2060-49a0-b0ec-cd0d4edce193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dostoevsky\n",
            "Но каково же было мое изумление, когда Наташа с первых же слов остановила меня и сказала, что нечего ее утешать, что она уже пять дней, как знает про это..     – Боже мой!\n",
            "['PER']\n",
            "PER\n",
            "[('ADV', 'NUM', 'NOUN', 'PUNCT', 'SCONJ'), ('VERB', 'PUNCT', 'SCONJ', 'PRON', 'ADV'), ('SCONJ', 'PRON', 'ADV', 'NUM', 'NOUN'), ('PRON', 'PUNCT', 'SPACE', 'PUNCT', 'NOUN'), ('CCONJ', 'VERB', 'PUNCT', 'SCONJ', 'VERB'), ('NOUN', 'VERB', 'PRON', 'CCONJ', 'VERB'), ('VERB', 'ADP', 'PRON', 'PUNCT', 'SPACE'), ('ADJ', 'PART', 'AUX', 'DET', 'NOUN'), ('PRON', 'VERB', 'PUNCT', 'SCONJ', 'PRON'), ('SCONJ', 'VERB', 'ADP', 'PRON', 'PUNCT'), ('CCONJ', 'ADJ', 'PART', 'AUX', 'DET'), ('PROPN', 'ADP', 'ADJ', 'PART', 'NOUN'), ('VERB', 'PRON', 'VERB', 'PUNCT', 'SCONJ'), ('AUX', 'DET', 'NOUN', 'PUNCT', 'SCONJ'), ('SCONJ', 'PROPN', 'ADP', 'ADJ', 'PART'), ('PRON', 'ADV', 'NUM', 'NOUN', 'PUNCT'), ('ADJ', 'PART', 'NOUN', 'VERB', 'PRON'), ('SCONJ', 'VERB', 'PRON', 'VERB', 'PUNCT'), ('PUNCT', 'SCONJ', 'VERB', 'ADP', 'PRON'), ('PUNCT', 'SCONJ', 'PRON', 'ADV', 'NUM'), ('SPACE', 'PUNCT', 'NOUN', 'DET', 'PUNCT'), ('NOUN', 'PUNCT', 'SCONJ', 'VERB', 'ADP'), ('VERB', 'PRON', 'CCONJ', 'VERB', 'PUNCT'), ('PART', 'NOUN', 'VERB', 'PRON', 'CCONJ'), ('VERB', 'PUNCT', 'SCONJ', 'VERB', 'PRON'), ('PART', 'AUX', 'DET', 'NOUN', 'PUNCT'), ('PUNCT', 'SCONJ', 'PROPN', 'ADP', 'ADJ'), ('DET', 'NOUN', 'PUNCT', 'SCONJ', 'PROPN'), ('ADP', 'PRON', 'PUNCT', 'SPACE', 'PUNCT'), ('NUM', 'NOUN', 'PUNCT', 'SCONJ', 'VERB'), ('PUNCT', 'SPACE', 'PUNCT', 'NOUN', 'DET'), ('NOUN', 'PUNCT', 'SCONJ', 'PROPN', 'ADP'), ('PUNCT', 'SCONJ', 'VERB', 'PRON', 'VERB'), ('PRON', 'CCONJ', 'VERB', 'PUNCT', 'SCONJ'), ('ADP', 'ADJ', 'PART', 'NOUN', 'VERB')]\n",
            "\n",
            "Gogol\n",
            "— закричали в толпе.. — Давай совет кошевой!\n",
            "[]\n",
            "[('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('NOUN', 'PUNCT', 'PUNCT', 'VERB', 'NOUN'), ('VERB', 'ADP', 'NOUN', 'PUNCT', 'PUNCT'), ('PUNCT', 'VERB', 'NOUN', 'PROPN', 'PUNCT'), ('ADP', 'NOUN', 'PUNCT', 'PUNCT', 'VERB'), ('PUNCT', 'PUNCT', 'VERB', 'NOUN', 'PROPN')]\n",
            "('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT')\n",
            "2\n",
            "\n",
            "Chekhov\n",
            "И всё, бывало, извиняется.\n",
            "[]\n",
            "[('CCONJ', 'PRON', 'PUNCT', 'VERB', 'PUNCT'), ('PUNCT', 'VERB', 'PUNCT', 'VERB', 'PUNCT'), ('PRON', 'PUNCT', 'VERB', 'PUNCT', 'VERB')]\n",
            "\n",
            "Chekhov\n",
            "Живу-ут!.\n",
            "[]\n",
            "[('VERB', 'NOUN', 'INTJ', 'PUNCT', 'PUNCT')]\n",
            "\n",
            "Tolstoy\n",
            "Там воду освятим: они скорее выздоровеют; и я теперь здоров: у меня болел глаз, а теперь смотрю в оба»..      — А мне говорили военные люди, — сказал Пьер, — что в городе никак нельзя сражаться и что позиция….\n",
            "['PER']\n",
            "PER\n",
            "[('VERB', 'CCONJ', 'SCONJ', 'NOUN', 'PUNCT'), ('ADP', 'PRON', 'VERB', 'NOUN', 'PUNCT'), ('SPACE', 'PUNCT', 'VERB', 'PROPN', 'PUNCT'), ('SPACE', 'PUNCT', 'SPACE', 'CCONJ', 'PRON'), ('ADJ', 'PUNCT', 'ADP', 'PRON', 'VERB'), ('ADP', 'NOUN', 'ADV', 'ADV', 'VERB'), ('PUNCT', 'ADP', 'PRON', 'VERB', 'NOUN'), ('VERB', 'PUNCT', 'CCONJ', 'PRON', 'ADV'), ('CCONJ', 'SCONJ', 'NOUN', 'PUNCT', 'PUNCT'), ('PRON', 'VERB', 'ADJ', 'NOUN', 'PUNCT'), ('PUNCT', 'SPACE', 'PUNCT', 'SPACE', 'CCONJ'), ('SPACE', 'PUNCT', 'SCONJ', 'ADP', 'NOUN'), ('NOUN', 'PUNCT', 'SPACE', 'PUNCT', 'VERB'), ('PRON', 'ADV', 'ADJ', 'PUNCT', 'ADP'), ('NOUN', 'VERB', 'PUNCT', 'PRON', 'ADV'), ('ADJ', 'NOUN', 'PUNCT', 'SPACE', 'PUNCT'), ('SCONJ', 'ADP', 'NOUN', 'ADV', 'ADV'), ('PUNCT', 'SCONJ', 'ADP', 'NOUN', 'ADV'), ('ADV', 'VERB', 'CCONJ', 'SCONJ', 'NOUN'), ('PRON', 'ADV', 'VERB', 'PUNCT', 'CCONJ'), ('PUNCT', 'SPACE', 'PUNCT', 'SCONJ', 'ADP'), ('PUNCT', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('ADV', 'ADV', 'VERB', 'CCONJ', 'SCONJ'), ('ADV', 'ADJ', 'PUNCT', 'ADP', 'PRON'), ('VERB', 'ADJ', 'NOUN', 'PUNCT', 'SPACE'), ('VERB', 'NOUN', 'PUNCT', 'CCONJ', 'ADV'), ('PROPN', 'PUNCT', 'SPACE', 'PUNCT', 'SCONJ'), ('VERB', 'ADP', 'NUM', 'PUNCT', 'PUNCT'), ('NUM', 'PUNCT', 'PUNCT', 'SPACE', 'PUNCT'), ('VERB', 'PUNCT', 'PRON', 'ADV', 'VERB'), ('PUNCT', 'PRON', 'ADV', 'VERB', 'PUNCT'), ('PUNCT', 'VERB', 'PROPN', 'PUNCT', 'SPACE'), ('CCONJ', 'PRON', 'ADV', 'ADJ', 'PUNCT'), ('CCONJ', 'PRON', 'VERB', 'ADJ', 'NOUN'), ('PRON', 'VERB', 'NOUN', 'PUNCT', 'CCONJ'), ('PUNCT', 'SPACE', 'CCONJ', 'PRON', 'VERB'), ('SPACE', 'CCONJ', 'PRON', 'VERB', 'ADJ'), ('NOUN', 'ADV', 'ADV', 'VERB', 'CCONJ'), ('ADV', 'VERB', 'PUNCT', 'CCONJ', 'PRON'), ('PUNCT', 'SPACE', 'PUNCT', 'VERB', 'PROPN'), ('ADV', 'VERB', 'ADP', 'NUM', 'PUNCT'), ('NOUN', 'PUNCT', 'CCONJ', 'ADV', 'VERB'), ('ADV', 'NOUN', 'VERB', 'PUNCT', 'PRON'), ('ADP', 'NUM', 'PUNCT', 'PUNCT', 'SPACE'), ('VERB', 'PROPN', 'PUNCT', 'SPACE', 'PUNCT'), ('PUNCT', 'CCONJ', 'PRON', 'ADV', 'ADJ'), ('CCONJ', 'ADV', 'VERB', 'ADP', 'NUM'), ('PUNCT', 'CCONJ', 'ADV', 'VERB', 'ADP')]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation test 2:"
      ],
      "metadata": {
        "id": "MhqxSCB4wBwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process a text\n",
        "train_features_matrix = numpy.zeros((len(train_sentences), 3 + len(fivegrams_list)))\n",
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "\n",
        "counter = 0\n",
        "# loop over each review, label and feature vector at the same time (zip)\n",
        "for sentence, author, feature_vector in zip(train_doc_sentences, train_authors, train_features_matrix):\n",
        "    print('Author:', author)\n",
        "    print(sentence)\n",
        "    NEs_in_sentence = [entity.label_ for entity in sentence.ents]\n",
        "    #print(tokens_list)\n",
        "    for entity_type in entity_types:\n",
        "      if entity_type in NEs_in_sentence:\n",
        "        entity_id = entity_types.index(entity_type)\n",
        "        print(entity_type)\n",
        "        print(entity_id)\n",
        "        feature_vector[entity_id] = 1\n",
        "        print(feature_vector)\n",
        "    fivegram_pos_list = fivegram_pos_extractor_from_sentence(sentence)\n",
        "    print(fivegram_pos_list)\n",
        "    for fivegram in fivegrams_list:\n",
        "      if fivegram in fivegram_pos_list:\n",
        "        fivegram_id = fivegrams_list.index(fivegram) + 3\n",
        "        print(fivegram_id)\n",
        "        print(fivegram)\n",
        "        feature_vector[fivegram_id] = 1\n",
        "        print(feature_vector)\n",
        "    print()\n",
        "    counter +=1\n",
        "    if counter == 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7YpUbRnvWee",
        "outputId": "d3ca6df5-5606-4bdb-f4af-9bff68dd5bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Dostoevsky\n",
            "Но каково же было мое изумление, когда Наташа с первых же слов остановила меня и сказала, что нечего ее утешать, что она уже пять дней, как знает про это..     – Боже мой!\n",
            "PER\n",
            "0\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[('ADV', 'NUM', 'NOUN', 'PUNCT', 'SCONJ'), ('VERB', 'PUNCT', 'SCONJ', 'PRON', 'ADV'), ('SCONJ', 'PRON', 'ADV', 'NUM', 'NOUN'), ('PRON', 'PUNCT', 'SPACE', 'PUNCT', 'NOUN'), ('CCONJ', 'VERB', 'PUNCT', 'SCONJ', 'VERB'), ('NOUN', 'VERB', 'PRON', 'CCONJ', 'VERB'), ('VERB', 'ADP', 'PRON', 'PUNCT', 'SPACE'), ('ADJ', 'PART', 'AUX', 'DET', 'NOUN'), ('PRON', 'VERB', 'PUNCT', 'SCONJ', 'PRON'), ('SCONJ', 'VERB', 'ADP', 'PRON', 'PUNCT'), ('CCONJ', 'ADJ', 'PART', 'AUX', 'DET'), ('PROPN', 'ADP', 'ADJ', 'PART', 'NOUN'), ('VERB', 'PRON', 'VERB', 'PUNCT', 'SCONJ'), ('AUX', 'DET', 'NOUN', 'PUNCT', 'SCONJ'), ('SCONJ', 'PROPN', 'ADP', 'ADJ', 'PART'), ('PRON', 'ADV', 'NUM', 'NOUN', 'PUNCT'), ('ADJ', 'PART', 'NOUN', 'VERB', 'PRON'), ('SCONJ', 'VERB', 'PRON', 'VERB', 'PUNCT'), ('PUNCT', 'SCONJ', 'VERB', 'ADP', 'PRON'), ('PUNCT', 'SCONJ', 'PRON', 'ADV', 'NUM'), ('SPACE', 'PUNCT', 'NOUN', 'DET', 'PUNCT'), ('NOUN', 'PUNCT', 'SCONJ', 'VERB', 'ADP'), ('VERB', 'PRON', 'CCONJ', 'VERB', 'PUNCT'), ('PART', 'NOUN', 'VERB', 'PRON', 'CCONJ'), ('VERB', 'PUNCT', 'SCONJ', 'VERB', 'PRON'), ('PART', 'AUX', 'DET', 'NOUN', 'PUNCT'), ('PUNCT', 'SCONJ', 'PROPN', 'ADP', 'ADJ'), ('DET', 'NOUN', 'PUNCT', 'SCONJ', 'PROPN'), ('ADP', 'PRON', 'PUNCT', 'SPACE', 'PUNCT'), ('NUM', 'NOUN', 'PUNCT', 'SCONJ', 'VERB'), ('PUNCT', 'SPACE', 'PUNCT', 'NOUN', 'DET'), ('NOUN', 'PUNCT', 'SCONJ', 'PROPN', 'ADP'), ('PUNCT', 'SCONJ', 'VERB', 'PRON', 'VERB'), ('PRON', 'CCONJ', 'VERB', 'PUNCT', 'SCONJ'), ('ADP', 'ADJ', 'PART', 'NOUN', 'VERB')]\n",
            "\n",
            "Author: Gogol\n",
            "— закричали в толпе.. — Давай совет кошевой!\n",
            "[('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('NOUN', 'PUNCT', 'PUNCT', 'VERB', 'NOUN'), ('VERB', 'ADP', 'NOUN', 'PUNCT', 'PUNCT'), ('PUNCT', 'VERB', 'NOUN', 'PROPN', 'PUNCT'), ('ADP', 'NOUN', 'PUNCT', 'PUNCT', 'VERB'), ('PUNCT', 'PUNCT', 'VERB', 'NOUN', 'PROPN')]\n",
            "5\n",
            "('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT')\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Author: Chekhov\n",
            "И всё, бывало, извиняется.\n",
            "[('CCONJ', 'PRON', 'PUNCT', 'VERB', 'PUNCT'), ('PUNCT', 'VERB', 'PUNCT', 'VERB', 'PUNCT'), ('PRON', 'PUNCT', 'VERB', 'PUNCT', 'VERB')]\n",
            "\n",
            "Author: Chekhov\n",
            "Живу-ут!.\n",
            "[('VERB', 'NOUN', 'INTJ', 'PUNCT', 'PUNCT')]\n",
            "\n",
            "Author: Tolstoy\n",
            "Там воду освятим: они скорее выздоровеют; и я теперь здоров: у меня болел глаз, а теперь смотрю в оба»..      — А мне говорили военные люди, — сказал Пьер, — что в городе никак нельзя сражаться и что позиция….\n",
            "PER\n",
            "0\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[('VERB', 'CCONJ', 'SCONJ', 'NOUN', 'PUNCT'), ('ADP', 'PRON', 'VERB', 'NOUN', 'PUNCT'), ('SPACE', 'PUNCT', 'VERB', 'PROPN', 'PUNCT'), ('SPACE', 'PUNCT', 'SPACE', 'CCONJ', 'PRON'), ('ADJ', 'PUNCT', 'ADP', 'PRON', 'VERB'), ('ADP', 'NOUN', 'ADV', 'ADV', 'VERB'), ('PUNCT', 'ADP', 'PRON', 'VERB', 'NOUN'), ('VERB', 'PUNCT', 'CCONJ', 'PRON', 'ADV'), ('CCONJ', 'SCONJ', 'NOUN', 'PUNCT', 'PUNCT'), ('PRON', 'VERB', 'ADJ', 'NOUN', 'PUNCT'), ('PUNCT', 'SPACE', 'PUNCT', 'SPACE', 'CCONJ'), ('SPACE', 'PUNCT', 'SCONJ', 'ADP', 'NOUN'), ('NOUN', 'PUNCT', 'SPACE', 'PUNCT', 'VERB'), ('PRON', 'ADV', 'ADJ', 'PUNCT', 'ADP'), ('NOUN', 'VERB', 'PUNCT', 'PRON', 'ADV'), ('ADJ', 'NOUN', 'PUNCT', 'SPACE', 'PUNCT'), ('SCONJ', 'ADP', 'NOUN', 'ADV', 'ADV'), ('PUNCT', 'SCONJ', 'ADP', 'NOUN', 'ADV'), ('ADV', 'VERB', 'CCONJ', 'SCONJ', 'NOUN'), ('PRON', 'ADV', 'VERB', 'PUNCT', 'CCONJ'), ('PUNCT', 'SPACE', 'PUNCT', 'SCONJ', 'ADP'), ('PUNCT', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('ADV', 'ADV', 'VERB', 'CCONJ', 'SCONJ'), ('ADV', 'ADJ', 'PUNCT', 'ADP', 'PRON'), ('VERB', 'ADJ', 'NOUN', 'PUNCT', 'SPACE'), ('VERB', 'NOUN', 'PUNCT', 'CCONJ', 'ADV'), ('PROPN', 'PUNCT', 'SPACE', 'PUNCT', 'SCONJ'), ('VERB', 'ADP', 'NUM', 'PUNCT', 'PUNCT'), ('NUM', 'PUNCT', 'PUNCT', 'SPACE', 'PUNCT'), ('VERB', 'PUNCT', 'PRON', 'ADV', 'VERB'), ('PUNCT', 'PRON', 'ADV', 'VERB', 'PUNCT'), ('PUNCT', 'VERB', 'PROPN', 'PUNCT', 'SPACE'), ('CCONJ', 'PRON', 'ADV', 'ADJ', 'PUNCT'), ('CCONJ', 'PRON', 'VERB', 'ADJ', 'NOUN'), ('PRON', 'VERB', 'NOUN', 'PUNCT', 'CCONJ'), ('PUNCT', 'SPACE', 'CCONJ', 'PRON', 'VERB'), ('SPACE', 'CCONJ', 'PRON', 'VERB', 'ADJ'), ('NOUN', 'ADV', 'ADV', 'VERB', 'CCONJ'), ('ADV', 'VERB', 'PUNCT', 'CCONJ', 'PRON'), ('PUNCT', 'SPACE', 'PUNCT', 'VERB', 'PROPN'), ('ADV', 'VERB', 'ADP', 'NUM', 'PUNCT'), ('NOUN', 'PUNCT', 'CCONJ', 'ADV', 'VERB'), ('ADV', 'NOUN', 'VERB', 'PUNCT', 'PRON'), ('ADP', 'NUM', 'PUNCT', 'PUNCT', 'SPACE'), ('VERB', 'PROPN', 'PUNCT', 'SPACE', 'PUNCT'), ('PUNCT', 'CCONJ', 'PRON', 'ADV', 'ADJ'), ('CCONJ', 'ADV', 'VERB', 'ADP', 'NUM'), ('PUNCT', 'CCONJ', 'ADV', 'VERB', 'ADP')]\n",
            "\n",
            "Author: Tolstoy\n",
            "Ее-то огромное состояние у него осталось теперь, а его собственное, родовое, перешло меньшому брату, князю Ивану, который теперь обер-гоф-кафермейстер (он назвал что-то в этом роде) и был министром.. .\n",
            "PER\n",
            "0\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[('PUNCT', 'ADJ', 'PUNCT', 'VERB', 'ADJ'), ('PUNCT', 'CCONJ', 'AUX', 'NOUN', 'PUNCT'), ('ADJ', 'NOUN', 'ADP', 'PRON', 'VERB'), ('ADJ', 'NOUN', 'PUNCT', 'NOUN', 'PROPN'), ('PRON', 'ADP', 'DET', 'NOUN', 'PUNCT'), ('NOUN', 'PUNCT', 'CCONJ', 'AUX', 'NOUN'), ('PRON', 'VERB', 'PRON', 'PRON', 'PRON'), ('ADV', 'PUNCT', 'CCONJ', 'DET', 'ADJ'), ('PRON', 'VERB', 'ADV', 'PUNCT', 'CCONJ'), ('ADV', 'NOUN', 'NOUN', 'NOUN', 'PROPN'), ('ADJ', 'PUNCT', 'VERB', 'ADJ', 'NOUN'), ('DET', 'ADJ', 'PUNCT', 'ADJ', 'PUNCT'), ('ADJ', 'PUNCT', 'ADJ', 'PUNCT', 'VERB'), ('PUNCT', 'VERB', 'ADJ', 'NOUN', 'PUNCT'), ('VERB', 'ADJ', 'NOUN', 'PUNCT', 'NOUN'), ('PUNCT', 'PRON', 'ADV', 'NOUN', 'NOUN'), ('DET', 'DET', 'DET', 'ADJ', 'NOUN'), ('PRON', 'PRON', 'PRON', 'ADP', 'DET'), ('VERB', 'PRON', 'PRON', 'PRON', 'ADP'), ('CCONJ', 'DET', 'ADJ', 'PUNCT', 'ADJ'), ('VERB', 'ADV', 'PUNCT', 'CCONJ', 'DET'), ('PROPN', 'PUNCT', 'PRON', 'ADV', 'NOUN'), ('DET', 'NOUN', 'PUNCT', 'CCONJ', 'AUX'), ('PUNCT', 'PRON', 'VERB', 'PRON', 'PRON'), ('PRON', 'ADV', 'NOUN', 'NOUN', 'NOUN'), ('NOUN', 'NOUN', 'NOUN', 'PROPN', 'PROPN'), ('ADP', 'PRON', 'VERB', 'ADV', 'PUNCT'), ('PROPN', 'PUNCT', 'PRON', 'VERB', 'PRON'), ('PUNCT', 'NOUN', 'PROPN', 'PUNCT', 'PRON'), ('ADP', 'DET', 'NOUN', 'PUNCT', 'CCONJ'), ('NOUN', 'PUNCT', 'NOUN', 'PROPN', 'PUNCT'), ('PUNCT', 'CCONJ', 'DET', 'ADJ', 'PUNCT'), ('NOUN', 'PROPN', 'PUNCT', 'PRON', 'ADV'), ('NOUN', 'NOUN', 'PROPN', 'PROPN', 'PUNCT'), ('PRON', 'PRON', 'ADP', 'DET', 'NOUN'), ('PROPN', 'PROPN', 'PUNCT', 'PRON', 'VERB'), ('NOUN', 'PROPN', 'PROPN', 'PUNCT', 'PRON'), ('DET', 'ADJ', 'NOUN', 'ADP', 'PRON'), ('CCONJ', 'AUX', 'NOUN', 'PUNCT', 'PUNCT'), ('DET', 'DET', 'ADJ', 'NOUN', 'ADP'), ('NOUN', 'ADP', 'PRON', 'VERB', 'ADV')]\n",
            "\n",
            "Author: Tolstoy\n",
            "— сказал он.\n",
            "[]\n",
            "\n",
            "Author: Gogol\n",
            "Он вступил на площадь не без какой-то невольной боязни, точно как будто сердце его предчувствовало что-то недоброе.\n",
            "[('ADJ', 'NOUN', 'PUNCT', 'ADV', 'SCONJ'), ('DET', 'ADJ', 'NOUN', 'PUNCT', 'ADV'), ('NOUN', 'PRON', 'VERB', 'PRON', 'PRON'), ('ADP', 'NOUN', 'PART', 'ADP', 'DET'), ('ADP', 'DET', 'DET', 'DET', 'ADJ'), ('PRON', 'VERB', 'PRON', 'PRON', 'PRON'), ('PART', 'NOUN', 'PRON', 'VERB', 'PRON'), ('NOUN', 'PART', 'ADP', 'DET', 'DET'), ('PRON', 'VERB', 'ADP', 'NOUN', 'PART'), ('DET', 'DET', 'DET', 'ADJ', 'NOUN'), ('NOUN', 'PUNCT', 'ADV', 'SCONJ', 'PART'), ('ADV', 'SCONJ', 'PART', 'NOUN', 'PRON'), ('VERB', 'PRON', 'PRON', 'PRON', 'ADJ'), ('VERB', 'ADP', 'NOUN', 'PART', 'ADP'), ('PART', 'ADP', 'DET', 'DET', 'DET'), ('SCONJ', 'PART', 'NOUN', 'PRON', 'VERB'), ('DET', 'DET', 'ADJ', 'NOUN', 'PUNCT'), ('PUNCT', 'ADV', 'SCONJ', 'PART', 'NOUN'), ('PRON', 'PRON', 'PRON', 'ADJ', 'PUNCT')]\n",
            "\n",
            "Author: Chekhov\n",
            "Господи, столько во всем этом жизни, поэзии, смысла, что камень бы тронулся, а я... я глуп и нелеп!».\n",
            "[('DET', 'PRON', 'NOUN', 'PUNCT', 'NOUN'), ('PUNCT', 'NOUN', 'PUNCT', 'SCONJ', 'NOUN'), ('PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT'), ('PUNCT', 'SCONJ', 'NOUN', 'AUX', 'VERB'), ('CCONJ', 'PRON', 'PUNCT', 'PRON', 'ADJ'), ('NOUN', 'AUX', 'VERB', 'PUNCT', 'CCONJ'), ('VERB', 'PUNCT', 'CCONJ', 'PRON', 'PUNCT'), ('NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN'), ('PUNCT', 'ADV', 'ADP', 'DET', 'PRON'), ('PUNCT', 'CCONJ', 'PRON', 'PUNCT', 'PRON'), ('PRON', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT'), ('ADP', 'DET', 'PRON', 'NOUN', 'PUNCT'), ('PRON', 'ADJ', 'CCONJ', 'ADJ', 'PUNCT'), ('CCONJ', 'ADJ', 'PUNCT', 'PUNCT', 'PUNCT'), ('PUNCT', 'PRON', 'ADJ', 'CCONJ', 'ADJ'), ('NOUN', 'PUNCT', 'SCONJ', 'NOUN', 'AUX'), ('SCONJ', 'NOUN', 'AUX', 'VERB', 'PUNCT'), ('AUX', 'VERB', 'PUNCT', 'CCONJ', 'PRON'), ('ADV', 'ADP', 'DET', 'PRON', 'NOUN'), ('ADJ', 'CCONJ', 'ADJ', 'PUNCT', 'PUNCT'), ('PRON', 'PUNCT', 'PRON', 'ADJ', 'CCONJ'), ('PROPN', 'PUNCT', 'ADV', 'ADP', 'DET'), ('NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'SCONJ')]\n",
            "\n",
            "Author: Chekhov\n",
            "Уходят..       Входит Чебутыкин..       Маша.\n",
            "PER\n",
            "0\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[('VERB', 'PROPN', 'PUNCT', 'SPACE', 'PROPN'), ('PROPN', 'PUNCT', 'SPACE', 'PROPN', 'PUNCT'), ('PUNCT', 'SPACE', 'VERB', 'PROPN', 'PUNCT'), ('SPACE', 'VERB', 'PROPN', 'PUNCT', 'SPACE'), ('VERB', 'PUNCT', 'SPACE', 'VERB', 'PROPN')]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing a function for feature vector modification"
      ],
      "metadata": {
        "id": "VVVGJxY2xWMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_feature_vectors(doc_sentences, features_matrix):\n",
        "  for sentence, feature_vector in zip(doc_sentences, features_matrix):\n",
        "    NEs_in_sentence = [entity.label_ for entity in sentence.ents]\n",
        "    for entity_type in entity_types:\n",
        "      if entity_type in NEs_in_sentence:\n",
        "        entity_id = entity_types.index(entity_type)\n",
        "        feature_vector[entity_id] = 1\n",
        "\n",
        "    fivegram_pos_list = fivegram_pos_extractor_from_sentence(sentence)\n",
        "    for fivegram in fivegrams_list:\n",
        "      if fivegram in fivegram_pos_list:\n",
        "        fivegram_id = fivegrams_list.index(fivegram) + 3\n",
        "        feature_vector[fivegram_id] = 1\n",
        "\n",
        "  return features_matrix"
      ],
      "metadata": {
        "id": "y_iykAuiwWtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_matrix = numpy.zeros((len(train_sentences), 3 + len(fivegrams_list)))\n",
        "train_doc_sentences = nlp.pipe(train_sentences)"
      ],
      "metadata": {
        "id": "t5FQiPUbxzbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_matrix_final = modify_feature_vectors(train_doc_sentences, train_features_matrix)"
      ],
      "metadata": {
        "id": "AG97nXT4x6Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "uCDUZxJwB6qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr7_NER_and_common_5grams = LogisticRegression()\n",
        "\n",
        "# Train the model on the data, storing the information learned from the dat`a\n",
        "# Model is learning the relationship between digits (x_train) and labels (y_train)\n",
        "lr7_NER_and_common_5grams.fit(train_features_matrix_final, train_authors)\n",
        "\n",
        "print(lr7_NER_and_common_5grams.classes_)\n",
        "print(lr7_NER_and_common_5grams.get_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqTj59hQx83a",
        "outputId": "dc45ba40-f393-4d7c-b031-88fc63162c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chekhov' 'Dostoevsky' 'Gogol' 'Tolstoy']\n",
            "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying the test set feature vectors"
      ],
      "metadata": {
        "id": "MHcqawY7yU4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_doc_sentences = nlp.pipe(test_sentences)\n",
        "test_features_matrix = numpy.zeros((len(test_sentences), 3 + len(fivegrams_list)))\n",
        "\n",
        "test_features_matrix_final = modify_feature_vectors(test_doc_sentences, test_features_matrix)"
      ],
      "metadata": {
        "id": "TYVmyjj6ya0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making predictions"
      ],
      "metadata": {
        "id": "Bfyn0z7wykfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(i):\n",
        "    print(test_sentences[i])\n",
        "    # print the features of the index\n",
        "    print(test_features_matrix_final[i])\n",
        "    # print the correct label of the index\n",
        "    print(test_authors[i])\n",
        "\n",
        "    print()\n",
        "    print(\"Prediction:\")\n",
        "    # print the prediction for the features of this index\n",
        "    print(lr7_NER_and_common_5grams.predict([test_features_matrix_final[i]]))\n",
        "    # print the probabilities for each label predictions\n",
        "    print(lr7_NER_and_common_5grams.predict_proba([test_features_matrix_final[i]]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "-Mi8Y0J5yiJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(0)\n",
        "predict(1)\n",
        "predict(2)\n",
        "predict(3)\n",
        "predict(4)\n",
        "predict(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfNtlX1sytD6",
        "outputId": "87c20e92-4a28-4252-82cd-f0df413f365c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Фома Фомич, говорю, разве это возможное дело?\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Dostoevsky\n",
            "\n",
            "Prediction:\n",
            "['Chekhov']\n",
            "[[0.2899068  0.21921092 0.2345566  0.25632568]]\n",
            "\n",
            "Пора бы уже домой.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Chekhov\n",
            "\n",
            "Prediction:\n",
            "['Dostoevsky']\n",
            "[[0.24315406 0.27003326 0.25959857 0.22721411]]\n",
            "\n",
            "А казаки все до одного прощались, зная, что много будет работы тем и другим, но не повершили, однако ж, тотчас разлучиться, а повершили дождаться темной ночной поры, чтоб не дать неприятелю увидеть убыль в казацком войске.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Gogol\n",
            "\n",
            "Prediction:\n",
            "['Gogol']\n",
            "[[0.19654238 0.16645477 0.36214207 0.27486078]]\n",
            "\n",
            "Вдруг слезы градом у обоих из глаз, дрогнули руки.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Dostoevsky\n",
            "\n",
            "Prediction:\n",
            "['Dostoevsky']\n",
            "[[0.24315406 0.27003326 0.25959857 0.22721411]]\n",
            "\n",
            "Но художник видел в этом нежном личике одну только заманчивую для кисти почти фарфоровую проэрачность тела, увлекательную легкую томность, тонкую светлую шейку и аристократическую легкостъ стана.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Gogol\n",
            "\n",
            "Prediction:\n",
            "['Dostoevsky']\n",
            "[[0.24315406 0.27003326 0.25959857 0.22721411]]\n",
            "\n",
            "Когда я в Берлине получил оттуда несколько маленьких писем, которые они уже успели мне написать, то тут только я и понял, как их любил.\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Dostoevsky\n",
            "\n",
            "Prediction:\n",
            "['Tolstoy']\n",
            "[[0.19672116 0.19283849 0.27259895 0.3378414 ]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions_NER_common_5grams = lr7_NER_and_common_5grams.predict(test_features_matrix_final)\n",
        "\n",
        "for p, r in zip(test_predictions_NER_common_5grams[:10], test_authors[:10]):\n",
        "    if p == r:\n",
        "        result = \"Correct\"\n",
        "    else:\n",
        "        result = \"Incorrect\"\n",
        "    print(p + \"(\" + result + \":\" + r + \")\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgUff3Vyyvg-",
        "outputId": "cb54211e-ede8-4577-da52-37a1941cd8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chekhov(Incorrect:Dostoevsky)\n",
            "Dostoevsky(Incorrect:Chekhov)\n",
            "Gogol(Correct:Gogol)\n",
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Dostoevsky(Incorrect:Gogol)\n",
            "Tolstoy(Incorrect:Dostoevsky)\n",
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Dostoevsky(Incorrect:Chekhov)\n",
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Dostoevsky(Correct:Dostoevsky)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "Gm1l7bT8zEpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to file in the current working directory\n",
        "pkl_filename = \"logreg7_NER_and_common_5grams.pkl\"\n",
        "with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(lr7_NER_and_common_5grams, file)"
      ],
      "metadata": {
        "id": "LveIaprty3w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "aP0_aFPLzWhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy model"
      ],
      "metadata": {
        "id": "sODsi91uzYn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_predictions = ['Dostoevsky'] * len(test_sentences)\n",
        "print(len(dummy_predictions))\n",
        "\n",
        "# Calculate the accuracy of these \"dummy predictions\"\n",
        "acc_dummy = accuracy_score(test_authors, dummy_predictions)\n",
        "print(f'The accuracy is: {acc_dummy}')\n",
        "print()\n",
        "print(classification_report(test_authors, dummy_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otw74QqgzO_I",
        "outputId": "e098b58e-e0ec-44b5-d545-e2eca5382c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "The accuracy is: 0.25\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Chekhov       0.00      0.00      0.00       250\n",
            "  Dostoevsky       0.25      1.00      0.40       250\n",
            "       Gogol       0.00      0.00      0.00       250\n",
            "     Tolstoy       0.00      0.00      0.00       250\n",
            "\n",
            "    accuracy                           0.25      1000\n",
            "   macro avg       0.06      0.25      0.10      1000\n",
            "weighted avg       0.06      0.25      0.10      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER + Common POS Model"
      ],
      "metadata": {
        "id": "ZqfZEEL_zgZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy:')\n",
        "\n",
        "acc = accuracy_score(test_authors, test_predictions_NER_common_5grams)\n",
        "print(acc)\n",
        "corr_count = accuracy_score(test_authors, test_predictions_NER_common_5grams, normalize=False)\n",
        "total_count = len(test_authors)\n",
        "\n",
        "print(f'Total reviews: {str(total_count)}')\n",
        "print(f'Total correct predictions: {str(corr_count)}')\n",
        "corr_ratio = corr_count / total_count\n",
        "print(f'Correct ratio: {str(corr_ratio)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ox_BdljzeNu",
        "outputId": "77640972-d5e2-42ff-fb61-0e38270b916c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:\n",
            "0.283\n",
            "Total reviews: 1000\n",
            "Total correct predictions:283\n",
            "Correct ratio:0.283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_authors, test_predictions_NER_common_5grams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-507yfLzyoT",
        "outputId": "943ebd9c-cb29-44f4-ba8c-ddfb037e82bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Chekhov       0.32      0.30      0.31       250\n",
            "  Dostoevsky       0.26      0.68      0.38       250\n",
            "       Gogol       0.36      0.07      0.11       250\n",
            "     Tolstoy       0.33      0.08      0.13       250\n",
            "\n",
            "    accuracy                           0.28      1000\n",
            "   macro avg       0.32      0.28      0.23      1000\n",
            "weighted avg       0.32      0.28      0.23      1000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}