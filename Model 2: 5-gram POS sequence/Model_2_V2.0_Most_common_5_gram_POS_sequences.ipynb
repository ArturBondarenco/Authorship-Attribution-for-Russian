{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data"
      ],
      "metadata": {
        "id": "5VUYi1CJI71O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries, downloading the model"
      ],
      "metadata": {
        "id": "Q9CN2W-tJB7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KihGAH5wGt8M",
        "outputId": "d24f9d58-2520-4b2c-a998-5089fef97a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.3\n",
            "1.2.2\n",
            "1.22.4\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "import sklearn\n",
        "import numpy\n",
        "\n",
        "print(pandas.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(numpy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Small Russian model:\n",
        "# !python -m spacy download ru_core_news_sm\n",
        "# nlp = spacy.load('ru_core_news_sm')\n",
        "\n",
        "# Large Russian model:\n",
        "!python -m spacy download ru_core_news_lg\n",
        "nlp = spacy.load('ru_core_news_lg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnqaCaQNJJTG",
        "outputId": "9ac6ebaa-3d43-4a94-dfd3-0b7f80f3385f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-19 14:52:54.363784: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_lg-3.5.0/ru_core_news_lg-3.5.0-py3-none-any.whl (513.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.4/513.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-lg==3.5.0) (3.5.2)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-lg==3.5.0)\n",
            "  Downloading pymorphy3-1.2.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-lg==3.5.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-lg==3.5.0) (2.1.2)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=afbf5f81c8be524294a0723702ba5f87468aae5b66d8908a976818c2e9a3196b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy3-dicts-ru, docopt, dawg-python, pymorphy3, ru-core-news-lg\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy3-1.2.0 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making lists and doc objects from csv files"
      ],
      "metadata": {
        "id": "ZkOq7P5lJOO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the training data from a csv file\n",
        "train_set = pandas.read_csv('./train_data.csv', encoding='utf-8')\n",
        "# train_set"
      ],
      "metadata": {
        "id": "J7YQL13sJLuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = pandas.read_csv('./test_data.csv', encoding='utf-8')\n",
        "# test_set"
      ],
      "metadata": {
        "id": "l1EeiRFcJY4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_set['text'].to_list()\n",
        "train_authors = train_set['author'].to_list()\n",
        "\n",
        "test_sentences = test_set['text'].to_list()\n",
        "test_authors = test_set['author'].to_list()\n",
        "\n",
        "print(len(train_authors), len(test_authors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L6k8K6cKOGT",
        "outputId": "1927bc62-2a0d-4773-f1e8-235ad9951515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "test_doc_sentences = nlp.pipe(test_sentences)"
      ],
      "metadata": {
        "id": "RgVWQ9VIKYfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the feature set"
      ],
      "metadata": {
        "id": "vUP7WOfiKeWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract data for Dostoyevsky\n",
        "dostoyevsky_data = train_set[train_set['author'] == 'Dostoevsky']['text'].to_list()\n",
        "\n",
        "# Extract data for Tolstoy\n",
        "tolstoy_data = train_set[train_set['author'] == 'Tolstoy']['text'].to_list()\n",
        "\n",
        "# Extract data for Chekhov\n",
        "chekhov_data = train_set[train_set['author'] == 'Chekhov']['text'].to_list()\n",
        "\n",
        "# Extract data for Gogol\n",
        "gogol_data = train_set[train_set['author'] == 'Gogol']['text'].to_list()\n",
        "\n",
        "dostoyevsky_data_doc = nlp.pipe(dostoyevsky_data)\n",
        "tolstoy_data_doc = nlp.pipe(tolstoy_data)\n",
        "chekhov_data_doc = nlp.pipe(chekhov_data)\n",
        "gogol_data_doc = nlp.pipe(gogol_data)"
      ],
      "metadata": {
        "id": "UYyzOFtHK--R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def fivegram_pos_count(list_of_doc_sentences):\n",
        "  n = 5\n",
        "  fivegram_pos_tags = []\n",
        "  for doc in list_of_doc_sentences:\n",
        "    # Iterate over each possible fivegram in the document\n",
        "    for i in range(len(doc) - n + 1):\n",
        "            # Extract the tokens for the current fivegram\n",
        "            fivegram_tokens = doc[i : i + n]\n",
        "            # Extract the POS tags of the tokens and add the POS tag combination to the list\n",
        "            fivegram_pos = tuple(token.pos_ for token in fivegram_tokens)\n",
        "            fivegram_pos_tags.append(fivegram_pos)\n",
        "  most_common_fivegrams = Counter(fivegram_pos_tags).most_common(5)\n",
        "  five_fivegrams_list = [element[0] for element in most_common_fivegrams]\n",
        "  return five_fivegrams_list\n",
        "\n",
        "def fivegram_pos_extractor_from_sentence(doc):\n",
        "    n = 5\n",
        "    fivegram_pos_tags = []\n",
        "    for i in range(len(doc) - n + 1):\n",
        "    # Extract the tokens for the current fivegram\n",
        "        fivegram_tokens = doc[i : i + n]\n",
        "        # Extract the POS tags of the tokens and add the POS tag combination to the list\n",
        "        fivegram_pos = tuple(token.pos_ for token in fivegram_tokens)\n",
        "        fivegram_pos_tags.append(fivegram_pos)\n",
        "    unique_fivegram_pos_tags = list(set(fivegram_pos_tags))\n",
        "\n",
        "    return unique_fivegram_pos_tags"
      ],
      "metadata": {
        "id": "6QDmw6XXKabM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "five_fivegrams_dostoyevsky = fivegram_pos_count(dostoyevsky_data_doc)\n",
        "five_fivegrams_tolstoy = fivegram_pos_count(tolstoy_data_doc)\n",
        "five_fivegrams_chekhov = fivegram_pos_count(chekhov_data_doc)\n",
        "five_fivegrams_gogol = fivegram_pos_count(gogol_data_doc)\n",
        "\n",
        "fivegrams_list = five_fivegrams_dostoyevsky + five_fivegrams_tolstoy + five_fivegrams_chekhov + five_fivegrams_gogol\n",
        "fivegrams_list = list(set(fivegrams_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjEyiF9hLGpT",
        "outputId": "1584e241-5fcf-4407-cc17-0263a0d2846b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(five_fivegrams_dostoyevsky)\n",
        "print(five_fivegrams_tolstoy)\n",
        "print(five_fivegrams_chekhov)\n",
        "print(five_fivegrams_gogol)\n",
        "print(len(fivegrams_list), fivegrams_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goj3u62DQxCV",
        "outputId": "043db33e-cbd0-4823-9f6a-835f0ad691cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('VERB', 'ADP', 'DET', 'NOUN', 'PUNCT'), ('PUNCT', 'SPACE', 'PUNCT', 'VERB', 'PRON'), ('NOUN', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('VERB', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('VERB', 'PRON', 'ADP', 'NOUN', 'PUNCT')]\n",
            "[('X', 'X', 'X', 'X', 'X'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('X', 'X', 'X', 'X', 'PUNCT'), ('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT')]\n",
            "[('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('PUNCT', 'PUNCT', 'VERB', 'PRON', 'PUNCT'), ('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('ADJ', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT')]\n",
            "[('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT')]\n",
            "14 [('NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('NOUN', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN'), ('ADJ', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT'), ('X', 'X', 'X', 'X', 'PUNCT'), ('VERB', 'PRON', 'ADP', 'NOUN', 'PUNCT'), ('X', 'X', 'X', 'X', 'X'), ('PUNCT', 'SPACE', 'PUNCT', 'VERB', 'PRON'), ('VERB', 'PUNCT', 'SPACE', 'PUNCT', 'SPACE'), ('VERB', 'ADP', 'DET', 'NOUN', 'PUNCT'), ('PUNCT', 'PUNCT', 'VERB', 'PRON', 'PUNCT'), ('NOUN', 'VERB', 'ADP', 'NOUN', 'PUNCT'), ('VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "# We are creating a matrix with zero vectors for each review (in training set and test set)\n",
        "train_features_matrix = numpy.zeros((len(train_sentences), len(fivegrams_list)))\n",
        "print(train_features_matrix.shape)\n",
        "\n",
        "test_features_matrix = numpy.zeros((len(test_sentences), len(fivegrams_list)))\n",
        "print(test_features_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paASne3ta1RN",
        "outputId": "9073b06f-34b7-4b33-886e-86e423de124a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 14)\n",
            "(1000, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying the feature vectors"
      ],
      "metadata": {
        "id": "HVdvDrFCbFmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation tests"
      ],
      "metadata": {
        "id": "HAUE7Jg6bhF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "\n",
        "for sentence, author in zip(train_doc_sentences, train_authors):\n",
        "    print(author)\n",
        "    print(sentence)\n",
        "    fivegram_pos_list = fivegram_pos_extractor_from_sentence(sentence)\n",
        "    print(fivegram_pos_list)\n",
        "    for fivegram in fivegrams_list:\n",
        "      if fivegram in fivegram_pos_list:\n",
        "        print(fivegram)\n",
        "        fivegram_id = fivegrams_list.index(fivegram)\n",
        "        print(fivegram_id)\n",
        "        sys.exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "r7y8j2Fka2vY",
        "outputId": "2b03012e-eb70-4e8c-95ed-aae237f14d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dostoevsky\n",
            "Но каково же было мое изумление, когда Наташа с первых же слов остановила меня и сказала, что нечего ее утешать, что она уже пять дней, как знает про это..     – Боже мой!\n",
            "[('VERB', 'PUNCT', 'SCONJ', 'VERB', 'PRON'), ('DET', 'NOUN', 'PUNCT', 'SCONJ', 'PROPN'), ('VERB', 'PRON', 'CCONJ', 'VERB', 'PUNCT'), ('SPACE', 'PUNCT', 'NOUN', 'DET', 'PUNCT'), ('PRON', 'PUNCT', 'SPACE', 'PUNCT', 'NOUN'), ('ADJ', 'PART', 'NOUN', 'VERB', 'PRON'), ('PROPN', 'ADP', 'ADJ', 'PART', 'NOUN'), ('ADP', 'PRON', 'PUNCT', 'SPACE', 'PUNCT'), ('PUNCT', 'SPACE', 'PUNCT', 'NOUN', 'DET'), ('SCONJ', 'PRON', 'ADV', 'NUM', 'NOUN'), ('NOUN', 'VERB', 'PRON', 'CCONJ', 'VERB'), ('ADJ', 'PART', 'AUX', 'DET', 'NOUN'), ('AUX', 'DET', 'NOUN', 'PUNCT', 'SCONJ'), ('PRON', 'CCONJ', 'VERB', 'PUNCT', 'SCONJ'), ('SCONJ', 'VERB', 'ADP', 'PRON', 'PUNCT'), ('VERB', 'PUNCT', 'SCONJ', 'PRON', 'ADV'), ('PART', 'AUX', 'DET', 'NOUN', 'PUNCT'), ('NOUN', 'PUNCT', 'SCONJ', 'PROPN', 'ADP'), ('PUNCT', 'SCONJ', 'VERB', 'ADP', 'PRON'), ('ADV', 'NUM', 'NOUN', 'PUNCT', 'SCONJ'), ('CCONJ', 'ADJ', 'PART', 'AUX', 'DET'), ('PART', 'NOUN', 'VERB', 'PRON', 'CCONJ'), ('NOUN', 'PUNCT', 'SCONJ', 'VERB', 'ADP'), ('VERB', 'PRON', 'VERB', 'PUNCT', 'SCONJ'), ('NUM', 'NOUN', 'PUNCT', 'SCONJ', 'VERB'), ('SCONJ', 'VERB', 'PRON', 'VERB', 'PUNCT'), ('VERB', 'ADP', 'PRON', 'PUNCT', 'SPACE'), ('SCONJ', 'PROPN', 'ADP', 'ADJ', 'PART'), ('CCONJ', 'VERB', 'PUNCT', 'SCONJ', 'VERB'), ('PRON', 'ADV', 'NUM', 'NOUN', 'PUNCT'), ('PUNCT', 'SCONJ', 'VERB', 'PRON', 'VERB'), ('PUNCT', 'SCONJ', 'PROPN', 'ADP', 'ADJ'), ('ADP', 'ADJ', 'PART', 'NOUN', 'VERB'), ('PRON', 'VERB', 'PUNCT', 'SCONJ', 'PRON'), ('PUNCT', 'SCONJ', 'PRON', 'ADV', 'NUM')]\n",
            "Gogol\n",
            "— закричали в толпе.. — Давай совет кошевой!\n",
            "[('VERB', 'ADP', 'NOUN', 'PUNCT', 'PUNCT'), ('PUNCT', 'PUNCT', 'VERB', 'NOUN', 'PROPN'), ('PUNCT', 'VERB', 'NOUN', 'PROPN', 'PUNCT'), ('ADP', 'NOUN', 'PUNCT', 'PUNCT', 'VERB'), ('NOUN', 'PUNCT', 'PUNCT', 'VERB', 'NOUN'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT')]\n",
            "('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT')\n",
            "13\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation test 2:"
      ],
      "metadata": {
        "id": "64GEL5lMcvbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "train_doc_sentences = nlp.pipe(train_sentences)\n",
        "\n",
        "for sentence, author, feature_vector in zip(train_doc_sentences, train_authors, train_features_matrix):\n",
        "    print('Author:', author)\n",
        "    print(sentence)\n",
        "    fivegram_pos_list = fivegram_pos_extractor_from_sentence(sentence)\n",
        "    print(fivegram_pos_list)\n",
        "    for fivegram in fivegrams_list:\n",
        "      if fivegram in fivegram_pos_list:\n",
        "        fivegram_id = fivegrams_list.index(fivegram)\n",
        "        print(fivegram_id)\n",
        "        print(fivegram)\n",
        "        feature_vector[fivegram_id] = 1\n",
        "        print(feature_vector.tolist())\n",
        "        sys.exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "BvC5rShWcKar",
        "outputId": "bf779abf-4fae-42b9-8355-5e4d387f55d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Dostoevsky\n",
            "Но каково же было мое изумление, когда Наташа с первых же слов остановила меня и сказала, что нечего ее утешать, что она уже пять дней, как знает про это..     – Боже мой!\n",
            "[('VERB', 'PUNCT', 'SCONJ', 'VERB', 'PRON'), ('DET', 'NOUN', 'PUNCT', 'SCONJ', 'PROPN'), ('VERB', 'PRON', 'CCONJ', 'VERB', 'PUNCT'), ('SPACE', 'PUNCT', 'NOUN', 'DET', 'PUNCT'), ('PRON', 'PUNCT', 'SPACE', 'PUNCT', 'NOUN'), ('ADJ', 'PART', 'NOUN', 'VERB', 'PRON'), ('PROPN', 'ADP', 'ADJ', 'PART', 'NOUN'), ('ADP', 'PRON', 'PUNCT', 'SPACE', 'PUNCT'), ('PUNCT', 'SPACE', 'PUNCT', 'NOUN', 'DET'), ('SCONJ', 'PRON', 'ADV', 'NUM', 'NOUN'), ('NOUN', 'VERB', 'PRON', 'CCONJ', 'VERB'), ('ADJ', 'PART', 'AUX', 'DET', 'NOUN'), ('AUX', 'DET', 'NOUN', 'PUNCT', 'SCONJ'), ('PRON', 'CCONJ', 'VERB', 'PUNCT', 'SCONJ'), ('SCONJ', 'VERB', 'ADP', 'PRON', 'PUNCT'), ('VERB', 'PUNCT', 'SCONJ', 'PRON', 'ADV'), ('PART', 'AUX', 'DET', 'NOUN', 'PUNCT'), ('NOUN', 'PUNCT', 'SCONJ', 'PROPN', 'ADP'), ('PUNCT', 'SCONJ', 'VERB', 'ADP', 'PRON'), ('ADV', 'NUM', 'NOUN', 'PUNCT', 'SCONJ'), ('CCONJ', 'ADJ', 'PART', 'AUX', 'DET'), ('PART', 'NOUN', 'VERB', 'PRON', 'CCONJ'), ('NOUN', 'PUNCT', 'SCONJ', 'VERB', 'ADP'), ('VERB', 'PRON', 'VERB', 'PUNCT', 'SCONJ'), ('NUM', 'NOUN', 'PUNCT', 'SCONJ', 'VERB'), ('SCONJ', 'VERB', 'PRON', 'VERB', 'PUNCT'), ('VERB', 'ADP', 'PRON', 'PUNCT', 'SPACE'), ('SCONJ', 'PROPN', 'ADP', 'ADJ', 'PART'), ('CCONJ', 'VERB', 'PUNCT', 'SCONJ', 'VERB'), ('PRON', 'ADV', 'NUM', 'NOUN', 'PUNCT'), ('PUNCT', 'SCONJ', 'VERB', 'PRON', 'VERB'), ('PUNCT', 'SCONJ', 'PROPN', 'ADP', 'ADJ'), ('ADP', 'ADJ', 'PART', 'NOUN', 'VERB'), ('PRON', 'VERB', 'PUNCT', 'SCONJ', 'PRON'), ('PUNCT', 'SCONJ', 'PRON', 'ADV', 'NUM')]\n",
            "Author: Gogol\n",
            "— закричали в толпе.. — Давай совет кошевой!\n",
            "[('VERB', 'ADP', 'NOUN', 'PUNCT', 'PUNCT'), ('PUNCT', 'PUNCT', 'VERB', 'NOUN', 'PROPN'), ('PUNCT', 'VERB', 'NOUN', 'PROPN', 'PUNCT'), ('ADP', 'NOUN', 'PUNCT', 'PUNCT', 'VERB'), ('NOUN', 'PUNCT', 'PUNCT', 'VERB', 'NOUN'), ('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT')]\n",
            "13\n",
            "('PUNCT', 'VERB', 'ADP', 'NOUN', 'PUNCT')\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing a function for feature vector modification"
      ],
      "metadata": {
        "id": "yLpC6JlrfjMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_feature_vectors(doc_sentences, features_matrix):\n",
        "  for sentence, feature_vector in zip(doc_sentences, features_matrix):\n",
        "    fivegram_pos_list = fivegram_pos_extractor_from_sentence(sentence)\n",
        "    for fivegram in fivegrams_list:\n",
        "      if fivegram in fivegram_pos_list:\n",
        "        fivegram_id = fivegrams_list.index(fivegram)\n",
        "        feature_vector[fivegram_id] = 1\n",
        "  return features_matrix"
      ],
      "metadata": {
        "id": "YDLiRGJtdDnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_matrix = numpy.zeros((len(train_sentences), len(fivegrams_list)))\n",
        "train_doc_sentences = nlp.pipe(train_sentences)"
      ],
      "metadata": {
        "id": "dFTbJjEUfshv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_matrix_final = modify_feature_vectors(train_doc_sentences, train_features_matrix)"
      ],
      "metadata": {
        "id": "ft387GuNfvWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Y29Elo9zf7aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_common_5grams = LogisticRegression()\n",
        "\n",
        "# Train the model on the data, storing the information learned from the dat`a\n",
        "# Model is learning the relationship between digits (x_train) and labels (y_train)\n",
        "lr_common_5grams.fit(train_features_matrix_final, train_authors)\n",
        "\n",
        "print(lr_common_5grams.classes_)\n",
        "print(lr_common_5grams.get_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCW9U5kYf6di",
        "outputId": "f7023f26-31b4-4a7c-cd47-affae115997e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chekhov' 'Dostoevsky' 'Gogol' 'Tolstoy']\n",
            "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying the test set feature vectors"
      ],
      "metadata": {
        "id": "kZKb4sgjgPvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_doc_sentences = nlp.pipe(test_sentences)\n",
        "test_features_matrix = numpy.zeros((len(test_sentences), len(fivegrams_list)))\n",
        "\n",
        "test_features_matrix_final = modify_feature_vectors(test_doc_sentences, test_features_matrix)"
      ],
      "metadata": {
        "id": "U_bTDnyOgYpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making predictions"
      ],
      "metadata": {
        "id": "jpgILNoOgiMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(i):\n",
        "    print(test_sentences[i])\n",
        "    # print the features of the index\n",
        "    print(test_features_matrix_final[i])\n",
        "    # print the correct label of the index\n",
        "    print(test_authors[i])\n",
        "\n",
        "    print()\n",
        "    print(\"Prediction:\")\n",
        "    # print the prediction for the features of this index\n",
        "    print(lr_common_5grams.predict([test_features_matrix_final[i]]))\n",
        "    # print the probabilities for each label predictions\n",
        "    print(lr_common_5grams.predict_proba([test_features_matrix_final[i]]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "5sh9nIY-gem0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(0)\n",
        "predict(1)\n",
        "predict(2)\n",
        "predict(3)\n",
        "predict(4)\n",
        "predict(5)"
      ],
      "metadata": {
        "id": "fDuBzukOgr0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions_common_5grams = lr_common_5grams.predict(test_features_matrix_final)\n",
        "\n",
        "for p, r in zip(test_predictions_common_5grams[:10], test_authors[:10]):\n",
        "    if p == r:\n",
        "        result = \"Correct\"\n",
        "    else:\n",
        "        result = \"Incorrect\"\n",
        "    print(p + \"(\" + result + \":\" + r + \")\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJSLnURGgt_k",
        "outputId": "68d4390d-23c2-4b12-cdaf-39a552035d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Dostoevsky(Incorrect:Chekhov)\n",
            "Gogol(Correct:Gogol)\n",
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Dostoevsky(Incorrect:Gogol)\n",
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Dostoevsky(Incorrect:Chekhov)\n",
            "Dostoevsky(Correct:Dostoevsky)\n",
            "Dostoevsky(Correct:Dostoevsky)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "vL9hSZTng4KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save to file in the current working directory\n",
        "pkl_filename = \"logreg_common_5grams.pkl\"\n",
        "with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(lr_common_5grams, file)"
      ],
      "metadata": {
        "id": "hyEJUUA8g0Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "0xtCvMdGhG7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy model"
      ],
      "metadata": {
        "id": "tAHlh_kThJX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "dummy_predictions = ['Dostoevsky'] * len(test_sentences)\n",
        "print(len(dummy_predictions))\n",
        "\n",
        "# Calculate the accuracy of these \"dummy predictions\"\n",
        "acc_dummy = accuracy_score(test_authors, dummy_predictions)\n",
        "print('The accuracy is:', acc_dummy)\n",
        "print()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_authors, dummy_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jCVjbFUhDzT",
        "outputId": "3891f081-2f31-46bb-9497-3066773e8a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "The accuracy is: 0.25\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Chekhov       0.00      0.00      0.00       250\n",
            "  Dostoevsky       0.25      1.00      0.40       250\n",
            "       Gogol       0.00      0.00      0.00       250\n",
            "     Tolstoy       0.00      0.00      0.00       250\n",
            "\n",
            "    accuracy                           0.25      1000\n",
            "   macro avg       0.06      0.25      0.10      1000\n",
            "weighted avg       0.06      0.25      0.10      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common POS Model"
      ],
      "metadata": {
        "id": "LidJ6L6fiC1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Accuracy:')\n",
        "\n",
        "acc = accuracy_score(test_authors, test_predictions_common_5grams)\n",
        "print(acc)\n",
        "corr_count = accuracy_score(test_authors, test_predictions_common_5grams, normalize=False)\n",
        "total_count = len(test_authors)\n",
        "\n",
        "print(\"Total reviews: \" + str(str(total_count)))\n",
        "print(\"Total correct predictions:\" + str(corr_count))\n",
        "corr_ratio = corr_count / total_count\n",
        "print(\"Correct ratio:\" + str(corr_ratio))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9Lkt8JDh_gK",
        "outputId": "8c1cf2c2-91ea-4658-cda7-d05ca293f58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:\n",
            "0.272\n",
            "Total reviews: 1000\n",
            "Total correct predictions:272\n",
            "Correct ratio:0.272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_authors, test_predictions_common_5grams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fc1djq-ikRT",
        "outputId": "7f9f53df-f437-4bba-c5ad-9ae05f271b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Chekhov       0.50      0.04      0.07       250\n",
            "  Dostoevsky       0.26      0.95      0.41       250\n",
            "       Gogol       0.31      0.07      0.12       250\n",
            "     Tolstoy       0.29      0.03      0.06       250\n",
            "\n",
            "    accuracy                           0.27      1000\n",
            "   macro avg       0.34      0.27      0.16      1000\n",
            "weighted avg       0.34      0.27      0.16      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kzw5GC4Cz-ld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}